{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "We are going to explore and study a basic and important task of Natural Language Processiing (NLP) which is text classification. This task can help us with various tasks for example sentiments analysis, langauge detection, text author detection and text labelling are only few to name. In this Notebook, we will implement text's langauge detection task using two NLP approaches. First approach is with statistics and we will developed very famous Naive Bayes classifier. Second approach is with Machine Learning and there we will implement a very simple single layer Long Short Term Memory (LSTM) model. We will use Accuracy to compare both models performance (we could also use F1-measure but I want to keep things simple). I used vanilla Python for Naive Bayes implementation and Pytorch for LSTM. The dataset I use is from [Kaggle](https://www.kaggle.com/basilb2s/language-detection) and it consist of text with their respective langauge label. The dataset contains text from 17 different languages, however I picked only 3 langauges with the most enteries (English, French and Spanish). I used 80% data for training and remaining 20% for validation.\n",
    "Any classification task can be further divided into binary or multi-class classification. Here as we picked 3 langauges so we are going to develop multi class classifier. Let's now start with Naive Bayes implementation.\n",
    "\n",
    "### Naive Bayes\n",
    "\n",
    "Naive Bayes is one of the important statistical algorithm for text classification. It has two assumptions:\n",
    "\n",
    "1. _Bag-of-words assumptions:_ Position of words in text does not matter and all the words are equally important. \n",
    "2. _Conditional Independence_: Features (words) probabilities $P(w|c)$ are independent of each other given class c.\n",
    "\n",
    "Naive Bayes is **naive** because of the **first** assumption. This is a strong assumption and **unrealistic for real data; however, this is very effective**. One thing two remember that both assumptions are incorrect, but they simplify the algorithm big time.\n",
    "\n",
    "Some notes from lectures of Dan Jurafsky (highly recommend watching [these](https://www.youtube.com/playlist?list=PLLssT5z_DsK8HbD2sPcUIDfQ7zmBarMYv) videos from him):\n",
    "- Small data ? Naive bayes is perfect\n",
    "- Reasonable amount of data? go with SVM or Logistic Regression\n",
    "- Huge data? Training ML models can be time consuming and here Naive Bayes will shine. Infact with enough data classifier wont even matter!\n",
    "\n",
    "Before diving into more details for Naive Bayes algorithm, lets analyze and prepare the dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 languages in dataset \n",
      "Total dataset count: 10337\n",
      "Total dataset using for this notebook count (3 languages): 3218\n",
      "\n",
      "\n",
      "Train set: \n",
      "\tTotat: 2700 \n",
      "\tEnglish: 1166, French: 850 Spanish: 684\n"
     ]
    }
   ],
   "source": [
    "# dataset courtesy: https://www.kaggle.com/basilb2s/language-detection\n",
    "ds = pd.read_csv('Language Detection.csv')\n",
    "\n",
    "print(f'{len(ds[\"Language\"].unique())} languages in dataset ')\n",
    "print(f'Total dataset count: {len(ds)}')\n",
    "\n",
    "\n",
    "np.random.shuffle(ds.values)\n",
    "total_ds = ds.loc[(ds['Language'] == 'Spanish') | (ds['Language'] == 'French') | (ds['Language'] == 'English')]\n",
    "print(f'Total dataset using for this notebook count (3 languages): {len(total_ds)}')\n",
    "\n",
    "no_of_docs = 2700\n",
    "train_ds = total_ds[:no_of_docs]\n",
    "valid_ds = total_ds[no_of_docs:]\n",
    "\n",
    "# Only considering three languages en, fr and es\n",
    "ds_en = train_ds.loc[train_ds['Language'] == 'English']\n",
    "ds_fr = train_ds.loc[train_ds['Language'] == 'French']\n",
    "ds_es = train_ds.loc[train_ds['Language'] == 'Spanish']\n",
    "\n",
    "print('\\n')\n",
    "print(f'Train set: \\n\\tTotat: {len(train_ds)} \\n\\tEnglish: {len(ds_en)}, French: {len(ds_fr)} Spanish: {len(ds_es)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Language</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>English</th>\n",
       "      <td>219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>French</th>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Spanish</th>\n",
       "      <td>135</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Text\n",
       "Language      \n",
       "English    219\n",
       "French     164\n",
       "Spanish    135"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Validation set: \\n\")\n",
    "valid_ds.groupby('Language').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating all rows into one corpus\n",
    "corpus_en = ds_en.Text.str.cat(sep=' ')\n",
    "corpus_fr = ds_fr.Text.str.cat(sep=' ')\n",
    "corpus_es = ds_es.Text.str.cat(sep=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have dataset for all three classes. Let's talk about our algorithm. So we want to find out probability of class _c_ given the document _d_ hence $P(c|d)$. According to the Bayes rule we can calculate it as\n",
    "\n",
    "$$ P(c|d) = \\frac{P(d|c)P(c)}{P(d)} $$\n",
    "\n",
    "Where $P(d)$ is the probability of the given document which is always constant (as we have fixed dataset) so we can drop it. Now it becomes\n",
    "\n",
    "$$ P(c|d) = P(d|c)P(c) $$\n",
    "\n",
    "Now $P(c)$ is probability of the class and we also call it **prior probability**. Lets calculate it first!\n",
    "\n",
    "\n",
    "$$ P(c) = \\frac{\\text{No. of c documents}}{\\text{Total no. of documents}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of en docs 1166 P(en): 0.4318518518518519\n",
      "No. of fr docs 850 P(fr): 0.3148148148148148\n",
      "No. of es docs 684 P(es): 0.25333333333333335\n"
     ]
    }
   ],
   "source": [
    "# Calculating prior probabilities\n",
    "prior_en = len(ds_en)/no_of_docs\n",
    "print(f\"No. of en docs {len(ds_en)} P(en): {prior_en}\")\n",
    "\n",
    "prior_fr = len(ds_fr)/no_of_docs\n",
    "print(f\"No. of fr docs {len(ds_fr)} P(fr): {prior_fr}\")\n",
    "\n",
    "prior_es = len(ds_es)/no_of_docs\n",
    "print(f\"No. of es docs {len(ds_es)} P(es): {prior_es}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our all three prior probabilities calculated. Its time to deal with $P(d|c)$ which we call **likelihood**. As _d_ is a document consists of many words _w_ we can rewrite as \n",
    "\n",
    "$$ P(c|d) = P(w_1 .... w_n|c)P(c) $$\n",
    "\n",
    "Now calculating $P(w_1 .... w_n|c)$ is not trivial, but because of the assumption of __*independence*__, joint probability will become\n",
    "\n",
    "\n",
    "$$ P(c|d) = P(w_1|c) P(w_2|c) ... P(w_n|c) P(c) $$\n",
    "\n",
    "Now the only calculation we need to do is for $P(w_i|c)$ and it can be calculated as\n",
    "\n",
    "$$P(w_i|c) = \\frac{ \\text{ No. of $w_i$ in c + 1} }{ \\text{No. of words in c + Vocab size} } $$\n",
    "\n",
    "Here the _Vocab size_ is the number of unique words in all three classes (the complete corpus). We added + 1 in the nominator and denominator to handle unknown words (It's called Laplace smoothing)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25427\n",
      "19589\n",
      "14809\n"
     ]
    }
   ],
   "source": [
    "# Pre-processing\n",
    "# Lower casing and tokenization i.e here we are simply splitting by spaces\n",
    "tokens_en = corpus_en.lower().split()\n",
    "tokens_fr = corpus_fr.lower().split()\n",
    "tokens_es = corpus_es.lower().split()\n",
    "print(len(tokens_en))\n",
    "print(len(tokens_fr))\n",
    "print(len(tokens_es))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size:  16995\n"
     ]
    }
   ],
   "source": [
    "# Creating vacabs\n",
    "vocab_en = Counter()\n",
    "vocab_fr = Counter()\n",
    "vocab_es = Counter()\n",
    "\n",
    "vocab_en.update(tokens_en)\n",
    "vocab_fr.update(tokens_fr)\n",
    "vocab_es.update(tokens_es)\n",
    "\n",
    "size_vocab_en = len(vocab_en)\n",
    "size_vocab_fr = len(vocab_fr)\n",
    "size_vocab_es = len(vocab_es)\n",
    "\n",
    "vocab_size = len(set(tokens_en)) + len(set(tokens_fr)) + len(set(tokens_es))\n",
    "\n",
    "print('Vocab size: ', vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nb(text, nb_component, vocab_size):\n",
    "    tokens = text.strip().lower().split()\n",
    "    res = nb_component.get('prior')\n",
    "    deno = nb_component.get('no_tokens') + vocab_size\n",
    "    for t in tokens:\n",
    "        nom = nb_component.get('vocab').get(t, 0) + 1\n",
    "        t_pred = (nom / deno)\n",
    "        res *= t_pred\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_components = {\n",
    "        'English': {'prior': prior_en, 'no_tokens': len(tokens_en), 'vocab': vocab_en },\n",
    "        'French': {'prior': prior_fr, 'no_tokens': len(tokens_fr), 'vocab': vocab_fr },\n",
    "        'Spanish': {'prior': prior_es, 'no_tokens': len(tokens_es), 'vocab': vocab_es },\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     English       0.98      1.00      0.99       219\n",
      "      French       1.00      0.99      1.00       164\n",
      "     Spanish       1.00      0.98      0.99       135\n",
      "\n",
      "    accuracy                           0.99       518\n",
      "   macro avg       0.99      0.99      0.99       518\n",
      "weighted avg       0.99      0.99      0.99       518\n",
      "\n"
     ]
    }
   ],
   "source": [
    "true_labels =[]\n",
    "pred_labels =[]\n",
    "\n",
    "for i, row in valid_ds.iterrows():\n",
    "    last_pred = -1\n",
    "    for lang in nb_components.keys():\n",
    "        nb_component = nb_components.get(lang)\n",
    "        pred = nb(row.Text, nb_component, vocab_size)\n",
    "\n",
    "        if pred > last_pred:\n",
    "            last_pred = pred\n",
    "            pred_lang = lang\n",
    "    true_labels.append(row.Language)\n",
    "    pred_labels.append(pred_lang)\n",
    "\n",
    "\n",
    "print(classification_report(true_labels, pred_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As our dataset is pretty imbalanced we have to consider `macro avg`. So with Naive Bayes, we are easily able to train robust classifier in no time! (Remember, here I did not to cross-validation and you should always do it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I know, I know it is very messy to implement Naive Bayes! \n",
    "> Basically, I tried to as explicit as possible with the implementation. So that we can see how we implement the theory! \n",
    "\n",
    "If you are interested here is `Scikit-learn` way!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99       219\n",
      "           1       0.99      1.00      1.00       164\n",
      "           2       1.00      0.97      0.98       135\n",
      "\n",
      "    accuracy                           0.99       518\n",
      "   macro avg       0.99      0.99      0.99       518\n",
      "weighted avg       0.99      0.99      0.99       518\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sklearn as sk\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "X_train = train_ds.Text\n",
    "Y_train = train_ds.Language\n",
    "\n",
    "vocab_train = CountVectorizer()\n",
    "X_train_encoded = vocab_train.fit_transform(X_train)\n",
    "Y_train_encoded = label_encoder.fit_transform(Y_train)\n",
    "\n",
    "nb_classifier = MultinomialNB().fit(X_train_encoded, Y_train_encoded)\n",
    "\n",
    "X_valid = valid_ds.Text\n",
    "Y_valid = valid_ds.Language\n",
    "\n",
    "X_valid_encoded = vocab_train.transform(X_valid)\n",
    "Y_valid_encoded = label_encoder.fit_transform(Y_valid)\n",
    "\n",
    "predicted = nb_classifier.predict(X_valid_encoded)\n",
    "\n",
    "print(classification_report(Y_valid_encoded, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning Approach\n",
    "\n",
    "So for Deep Learning we will use LSTM. I would not explain all the technicalities invovled in LSTM and deep learning (e.g batch, optimizer etc), rather I would discuss how to implement LSTM with Pytorch and how Deep Learning can help us to improve classification performance!\n",
    "However, I would highly recommend very easily explained article from Olah _[Understanding LSTM](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)_. \n",
    "Lets import Pytorch's modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp_params = {\n",
    "    \"batch_size\": 64,\n",
    "    \"embedding_dim\": 125,\n",
    "    \"hidden_dim\": 10\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as always, first step is to prepare data. We would create a parallel (source and target) kind of dataset where source is the text and target is the langauge of the source text. In Pytorch, we have to create a `Class` for dataset which should be inhereted from `Dataset` class provided by Pytorch and it must implement three functions: `__init__, __len__, and __getitem__`. `__len__` should return number of rows in our datasets and `__getitem__` should return a pair of `source` and `target`. We will also generate vocabulary of our dataset and encode our text and language using this vocabulary. Now let's code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LangDataset(Dataset):\n",
    "    def __init__(self, ds, train_vocab=None):\n",
    "        self.corpus = ds\n",
    "\n",
    "        if not train_vocab:\n",
    "            self.src_vocab, self.trg_vocab = self._build_vocab()\n",
    "        else:\n",
    "            self.src_vocab, self.trg_vocab = train_vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.corpus)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        text = self.corpus.iloc[item].Text\n",
    "        lang = self.corpus.iloc[item].Language\n",
    "\n",
    "        return {\n",
    "            'src': self.src_vocab.lookup_indices(text.lower().split()),\n",
    "            'trg': self.trg_vocab.lookup_indices([lang])\n",
    "        }\n",
    "\n",
    "    def _build_vocab(self):\n",
    "        src_tokens = self.corpus.Text.str.cat().lower().split()\n",
    "\n",
    "        src_vocab = build_vocab_from_iterator([src_tokens], specials=[\"<unk>\",\"<pad>\"])\n",
    "        src_vocab.set_default_index(src_vocab['<unk>'])\n",
    "\n",
    "        trg_vocab = build_vocab_from_iterator([self.corpus.Language.unique().tolist()])\n",
    "\n",
    "        return src_vocab, trg_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we only need to make batches then we would ready for model's training. To ease creation of batches, Pytorch provides really handy `DataLoader` class. We need to instantiate this class and provide `dataset` class, batch size and a collate function. So now what is collate function?\n",
    "Collate function is a way to customize data batch process and shuffling. For this tutorial, we could avoid it and let Pytorch use its default collate function but learning it comes very handy in complex situation. So here we are just creating batch ourself, putting tensors on device and also padding them so that they are equal in length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch, pad_value, device):\n",
    "    trgs = []\n",
    "    srcs = []\n",
    "    for row in batch:\n",
    "        srcs.append(torch.tensor(row[\"src\"], dtype=torch.long).to(device))\n",
    "        trgs.append(torch.tensor(row[\"trg\"]).to(device))\n",
    "\n",
    "    padded_srcs = pad_sequence(srcs, padding_value=pad_value)\n",
    "    return {\"src\": padded_srcs, \"trg\": torch.tensor(trgs).to(device)}\n",
    "    \n",
    "train_langds = LangDataset(train_ds)\n",
    "valid_langds = LangDataset(valid_ds, (train_langds.src_vocab, train_langds.trg_vocab))\n",
    "\n",
    "pad_value = train_langds.src_vocab['<pad>']\n",
    "\n",
    "train_dt = DataLoader(train_langds, batch_size=hyp_params[\"batch_size\"], shuffle=\n",
    "                   True, collate_fn=lambda batch_size: collate_fn(batch_size, pad_value, device))\n",
    "\n",
    "valid_dt = DataLoader(valid_langds, batch_size=hyp_params[\"batch_size\"], shuffle=\n",
    "                   True, collate_fn=lambda batch_size: collate_fn(batch_size, pad_value, device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, its time to talk about our classification model. Its really simple its has a embedding layer, then LSTM layer and at the end a linear layer. Embedding layer takes our tensors and return word embedding for them. Then we feed these embeddings to LSTM and finally we send hidden state of LSTM to a linear layer!\n",
    "\n",
    "The question we should ask here is why we are not using output of LSTM for predictions? (I would try my best to not to go in theoritical details and explain clearly) Actually LSTM is a recurrent neural network and these network are designed to process sequential data. So LSTM takes each word at a time and produce output. So we have output for every input in the output of LSTM which we definitly we dont want. Rather, we want output after the complete sentence. So in `hidden_state` we have result of last token and this result was calculated using all the previous tokens as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        # Embedding is just an lookup table of size \"vocab_size\"\n",
    "        # and each element has \"embedding_size\" dimension\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.LSTM = nn.LSTM(embedding_dim, hidden_dim)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Shape --> [Sequence_length , batch_size , embedding dims]\n",
    "        embedding = self.embedding(x)\n",
    "        # Shape --> (output) [Sequence_length, batch_size, hidden_size]\n",
    "        # Shape --> (hs, cs) [num_layers, batch_size, hidden_size]\n",
    "        outputs, (hidden_state, cell_state) = self.LSTM(embedding)\n",
    "\n",
    "        linear_outputs = self.fc(hidden_state)\n",
    "\n",
    "        return linear_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters\n",
    "Before going to the training details, I would like to mention that `batch size`, `embedding dim` and `hidden dim` are hypermeters. One can experiment with different values of them and may find better accuracy (fine-tuning).\n",
    "\n",
    "As we are doing multi-class classification, so we will use `CrossEntropyLoss`. Now lets train our model for just 10 epochs and see: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "\tTrain loss:  1.0854351187861242 Eval loss:  1.0830692052841187\n",
      "Epoch:  1\n",
      "\tTrain loss:  1.0741313640461412 Eval loss:  1.0677900248103671\n",
      "Epoch:  2\n",
      "\tTrain loss:  1.0746076606040778 Eval loss:  1.0767928229437933\n",
      "Epoch:  3\n",
      "\tTrain loss:  1.072192409703898 Eval loss:  1.0754949516720242\n",
      "Epoch:  4\n",
      "\tTrain loss:  1.0562008605446926 Eval loss:  1.0316102372275457\n",
      "Epoch:  5\n",
      "\tTrain loss:  0.972022928470789 Eval loss:  0.9751913812425401\n",
      "Epoch:  6\n",
      "\tTrain loss:  0.9080666206603827 Eval loss:  0.9193954004181756\n",
      "Epoch:  7\n",
      "\tTrain loss:  0.8565663833950841 Eval loss:  0.9071177774005466\n",
      "Epoch:  8\n",
      "\tTrain loss:  0.7645499345868133 Eval loss:  0.7576818068822225\n",
      "Epoch:  9\n",
      "\tTrain loss:  0.7548888311829678 Eval loss:  0.7708468437194824\n"
     ]
    }
   ],
   "source": [
    "model = Classifier(len(train_langds.src_vocab), len(train_langds.trg_vocab), hyp_params[\"embedding_dim\"], hyp_params[\"hidden_dim\"])\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    print('Epoch: ', epoch)\n",
    "    for idx, batch in enumerate(train_dt):\n",
    "        src = batch[\"src\"]  # shape --> e.g. (19, 2) sentence len, batch size\n",
    "        trg = batch[\"trg\"]  # shape --> e.g. (1, 2) 1, batch size\n",
    "\n",
    "        # Clear the accumulating gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # shape --> (1, 32, 3) 1, batch size, trg vocab\n",
    "        output = model(src)\n",
    "\n",
    "        # Calculate the loss value for every epoch\n",
    "        # Squeezing to remove first dimension \n",
    "        loss = criterion(output.squeeze(0), trg.squeeze(0))\n",
    "\n",
    "        # Calculate the gradients for weights & biases using back-propagation\n",
    "        loss.backward()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # Clip the gradient value is it exceeds > 1\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "\n",
    "        # Update the weights values\n",
    "        optimizer.step()\n",
    "    print('\\tTrain loss: ', epoch_loss/len(train_dt), end=\" \")\n",
    "\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(valid_dt):\n",
    "            src = batch[\"src\"]  # shape --> e.g. (19, 2) sentence len, batch size\n",
    "            trg = batch[\"trg\"]  # shape --> e.g. (3, 2) sentence len, batch size\n",
    "\n",
    "            output = model(src)\n",
    "\n",
    "            # Calculate the loss value for every epoch\n",
    "            loss = criterion(output.squeeze(0), trg.squeeze(0))\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    print('Eval loss: ', epoch_loss/len(valid_dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Classifier(\n",
       "  (embedding): Embedding(16264, 125)\n",
       "  (LSTM): LSTM(125, 10)\n",
       "  (fc): Linear(in_features=10, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.53      0.65       219\n",
      "           1       0.50      0.85      0.63       164\n",
      "           2       0.93      0.73      0.82       135\n",
      "\n",
      "    accuracy                           0.68       518\n",
      "   macro avg       0.76      0.70      0.70       518\n",
      "weighted avg       0.76      0.68      0.69       518\n",
      "\n"
     ]
    }
   ],
   "source": [
    "true_labels =[]\n",
    "pred_labels =[]\n",
    "\n",
    "for i in valid_langds:\n",
    "    inp = torch.tensor(i['src']).unsqueeze(1)\n",
    "    trg = i['trg'][0]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred = model(inp).view(-1).argmax().item()\n",
    "\n",
    "    true_labels.append(trg)\n",
    "    pred_labels.append(pred)\n",
    "\n",
    "print(classification_report(true_labels, pred_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that it is bit more difficult to code a LSTM model and it takes time to train, and in our case we only able to score F1-score of 70. However, with more epochs and hyperparameter tunning LSTM can be very robust classifier.\n",
    "Also, we have simple classification problem at hand, LSTM can out-class Naive Bayes on many different classification problems. However, we can not underestimate the power of simple Naive Bayes!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'English'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_map = {0: 'English', 1: 'French', 2: 'Spanish'}\n",
    "\n",
    "text = \"hello, how are you?\"\n",
    "txt_to_ind = train_langds.src_vocab.lookup_indices(text.split())\n",
    "inp_tensor = torch.tensor(txt_to_ind).unsqueeze(1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    res = model(inp_tensor).view(-1).argmax().item()\n",
    "out_map[res]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
