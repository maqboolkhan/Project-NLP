{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification\n",
    "\n",
    "We are going to explore and study a basic and important task of Natural Language Processiing (NLP) which is text classification. This task can help us with various tasks for example sentiments analysis, langauge detection, text author detection and text labelling are only few to name. In this Notebook, we will implement text's langauge detection task using two NLP approaches. First approach is with statistics and we will developed very famous Naive Bayes classifier. Second approach is with Machine Learning and there we will implement a very simple single layer Long Short Term Memory (LSTM) model. We will use Accuracy to compare both models performance (we could also use F1-measure but I want to keep things simple). I used vanilla Python for Naive Bayes implementation and Pytorch for LSTM. The dataset I use is from [Kaggle](https://www.kaggle.com/basilb2s/language-detection) and it consist of text with their respective langauge label. The dataset contains text from 17 different languages, however I picked only 3 langauges with the most enteries (English, French and Spanish). I used 80% data for training and remaining 20% for validation.\n",
    "Any classification task can be further divided into binary or multi-class classification. Here as we picked 3 langauges so we are going to develop multi class classifier. Let's now start with Naive Bayes implementation.\n",
    "\n",
    "### Naive Bayes\n",
    "\n",
    "\n",
    "- If you have data go with rule based\n",
    "- small data ? Naive bayes is perfect\n",
    "- Reasonable amount of data? go with SVM or Logistic Regression\n",
    "- Huge data? Training ML models can be time consuming and here Naive Bayes will shine. Infact with enough data classifier wont even matter!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 languages in dataset \n",
      "Total dataset count: 10337\n",
      "{'English': 1385, 'Malayalam': 594, 'Hindi': 63, 'Tamil': 469, 'Portugeese': 739, 'French': 1014, 'Dutch': 546, 'Spanish': 819, 'Greek': 365, 'Russian': 692, 'Danish': 428, 'Italian': 698, 'Turkish': 474, 'Sweedish': 676, 'Arabic': 536, 'German': 470, 'Kannada': 369}\n",
      "Total dataset count (3 languages): 3218\n"
     ]
    }
   ],
   "source": [
    "# dataset courtesy: https://www.kaggle.com/basilb2s/language-detection\n",
    "# https://github.com/jacoxu/StackOverflow\n",
    "ds = pd.read_csv('Language Detection.csv')\n",
    "\n",
    "print(f'{len(ds[\"Language\"].unique())} languages in dataset ')\n",
    "print(f'Total dataset count: {len(ds)}')\n",
    "\n",
    "stats = { lang: len(ds.loc[ds['Language'] == lang])  for lang in ds[\"Language\"].unique()  }\n",
    "print(stats)\n",
    "\n",
    "np.random.shuffle(ds.values)\n",
    "total_ds = ds.loc[(ds['Language'] == 'Spanish') | (ds['Language'] == 'French') | (ds['Language'] == 'English')]\n",
    "print(f'Total dataset count (3 languages): {len(total_ds)}')\n",
    "\n",
    "train_ds = total_ds[:2700]\n",
    "valid_ds = total_ds[2700:]\n",
    "\n",
    "# Only considering three languages en, fr and es\n",
    "ds_en = train_ds.loc[ds['Language'] == 'English']\n",
    "ds_fr = train_ds.loc[ds['Language'] == 'French']\n",
    "ds_es = train_ds.loc[ds['Language'] == 'Spanish']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no of docs: 2700\n"
     ]
    }
   ],
   "source": [
    "# Total number of docs\n",
    "no_of_docs = len(ds_en) + len(ds_fr) + len(ds_es) \n",
    "\n",
    "print(f\"Total no of docs: {no_of_docs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of en docs 1170 P(en): 0.43333333333333335\n",
      "No. of fr docs 852 P(fr): 0.31555555555555553\n",
      "No. of es docs 678 P(es): 0.2511111111111111\n"
     ]
    }
   ],
   "source": [
    "# Calculating prior probabilities\n",
    "prior_en = len(ds_en)/no_of_docs\n",
    "print(f\"No. of en docs {len(ds_en)} P(en): {prior_en}\")\n",
    "\n",
    "prior_fr = len(ds_fr)/no_of_docs\n",
    "print(f\"No. of fr docs {len(ds_fr)} P(fr): {prior_fr}\")\n",
    "\n",
    "prior_es = len(ds_es)/no_of_docs\n",
    "print(f\"No. of es docs {len(ds_es)} P(es): {prior_es}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenating all rows into one corpus\n",
    "corpus_en = ds_en.Text.str.cat()\n",
    "corpus_fr = ds_fr.Text.str.cat()\n",
    "corpus_es = ds_es.Text.str.cat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24523\n",
      "18486\n",
      "13709\n"
     ]
    }
   ],
   "source": [
    "# Pre-processing\n",
    "# Lower casing and tokenization i.e here we are simply splitting by spaces\n",
    "tokens_en = corpus_en.lower().split()\n",
    "tokens_fr = corpus_fr.lower().split()\n",
    "tokens_es = corpus_es.lower().split()\n",
    "print(len(tokens_en))\n",
    "print(len(tokens_fr))\n",
    "print(len(tokens_es))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24523\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating vacabs\n",
    "vocab_en = Counter()\n",
    "vocab_fr = Counter()\n",
    "vocab_es = Counter()\n",
    "\n",
    "vocab_en.update(tokens_en)\n",
    "vocab_fr.update(tokens_fr)\n",
    "vocab_es.update(tokens_es)\n",
    "\n",
    "size_vocab_en = len(vocab_en) \n",
    "size_vocab_fr = len(vocab_fr) \n",
    "size_vocab_es = len(vocab_es) \n",
    "\n",
    "print(len(tokens_en))\n",
    "print()\n",
    "\n",
    "# plus one for unknown word\n",
    "vocab_size = len(set(tokens_en)) + len(set(tokens_fr)) + len(set(tokens_es)) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nb(text, cls_w_count, cls_prior, cls_vocab, vocab_size):\n",
    "    tokens = text.strip().lower().split()\n",
    "    res = cls_prior\n",
    "    dino = cls_w_count + vocab_size\n",
    "    \n",
    "    for t in tokens:\n",
    "        nom = cls_vocab.get(t, 0) + 1\n",
    "        res *= (nom / dino)\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.2722007722007722\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "langs = {\n",
    "        'English': {'prior': prior_en, 'tokens': len(tokens_en), 'vocab': vocab_en },\n",
    "        'French': {'prior': prior_fr, 'tokens': len(tokens_fr), 'vocab': vocab_fr },\n",
    "        'Spanish': {'prior': prior_es, 'tokens': len(tokens_es), 'vocab': vocab_es },\n",
    "    }\n",
    "\n",
    "last_pred = -1\n",
    "\n",
    "total = len(valid_ds)\n",
    "correct = 0\n",
    "\n",
    "for i, row in valid_ds.iterrows():\n",
    "    text = row.Text\n",
    "    label = row.Language\n",
    "    for lang in langs.keys():\n",
    "        lang_obj = langs.get(lang)\n",
    "        prior = lang_obj.get('prior')\n",
    "        pred = nb(text, lang_obj.get('tokens'), prior, lang_obj.get('vocab'), vocab_size)\n",
    "        if pred > last_pred:\n",
    "            lp = pred\n",
    "            pred_lang = lang\n",
    "    if pred_lang == label:\n",
    "        correct += 1\n",
    "\n",
    "print('Accuracy: ', correct/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/home/trtm/torch_dev/bin/python3\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import Dataset\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tqdm import tqdm\n",
    "\n",
    "class LangDataset(Dataset):\n",
    "    def __init__(self, ds, train_vocab=None):\n",
    "        self.corpus = ds\n",
    "\n",
    "        if not train_vocab:\n",
    "            self.src_vocab, self.trg_vocab = self._build_vocab()\n",
    "        else:\n",
    "            self.src_vocab, self.trg_vocab = train_vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.corpus)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        \n",
    "        lang = self.corpus.iloc[item].Language\n",
    "        text = self.corpus.iloc[item].Text\n",
    "        \n",
    "        return {\n",
    "            'src': self.src_vocab.lookup_indices(text.lower().split()),\n",
    "            'trg': self.trg_vocab.lookup_indices([lang])\n",
    "        }\n",
    "    \n",
    "    def _build_vocab(self):\n",
    "        src_tokens = self.corpus.Text.str.cat().lower().split()\n",
    "        \n",
    "        src_vocab = build_vocab_from_iterator([src_tokens], specials=[\"<unk>\",\"<pad>\"])\n",
    "        src_vocab.set_default_index(src_vocab['<unk>'])\n",
    "        \n",
    "        trg_vocab = build_vocab_from_iterator([['English', 'French', 'Spanish']])\n",
    "\n",
    "        return src_vocab, trg_vocab\n",
    "        \n",
    "    \n",
    "def collate_fn(batch, device):\n",
    "    trgs = []\n",
    "    srcs = []\n",
    "    for row in batch:\n",
    "        srcs.append(torch.tensor(row[\"src\"], dtype=torch.long).to(device))\n",
    "        trgs.append(torch.tensor(row[\"trg\"]).to(device))\n",
    "\n",
    "    padded_srcs = pad_sequence(srcs, padding_value=1)\n",
    "    padded_trgs = pad_sequence(trgs, padding_value=1)\n",
    "    return {\"src\": padded_srcs, \"trg\": padded_trgs}\n",
    "    \n",
    "train_langds = LangDataset(train_ds)\n",
    "valid_langds = LangDataset(valid_ds, (train_langds.src_vocab, train_langds.trg_vocab))\n",
    "\n",
    "\n",
    "train_dt = DataLoader(train_langds, batch_size=32, shuffle=\n",
    "                   True, collate_fn=lambda batch_size: collate_fn(batch_size, device))\n",
    "\n",
    "valid_dt = DataLoader(valid_langds, batch_size=32, shuffle=\n",
    "                   True, collate_fn=lambda batch_size: collate_fn(batch_size, device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, vocab_size, output_size, embedding_size, hidden_size):\n",
    "        super(Classifier, self).__init__()\n",
    "\n",
    "\n",
    "        # Embedding is just an lookup table of size \"vocab_size\"\n",
    "        # and each element has \"embedding_size\" dimension\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.LSTM = nn.LSTM(embedding_size, hidden_size)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        self.act = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Shape --> [Sequence_length , batch_size , embedding dims]\n",
    "        embedding = self.embedding(x)\n",
    "        # Shape --> (output) [Sequence_length , batch_size , hidden_size]\n",
    "        # Shape --> (hs, cs) [num_layers, batch_size size, hidden_size]\n",
    "        outputs, (hidden_state, cell_state) = self.LSTM(embedding)\n",
    "        \n",
    "        linear_outputs = self.fc(hidden_state)\n",
    "        \n",
    "        return linear_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "\tTrain loss:  1.0750935905119952\n",
      "\tEval loss:  1.0514680883463692\n",
      "Epoch:  1\n",
      "\tTrain loss:  0.9848196240032421\n",
      "\tEval loss:  0.975800030371722\n",
      "Epoch:  2\n",
      "\tTrain loss:  0.9092971254797543\n",
      "\tEval loss:  0.9175489439683802\n",
      "Epoch:  3\n",
      "\tTrain loss:  0.8410687951480641\n",
      "\tEval loss:  0.8594990302534664\n",
      "Epoch:  4\n",
      "\tTrain loss:  0.7871780760147993\n",
      "\tEval loss:  0.8522900798741508\n",
      "Epoch:  5\n",
      "\tTrain loss:  0.73675176115597\n",
      "\tEval loss:  0.7715242785565993\n",
      "Epoch:  6\n",
      "\tTrain loss:  0.678215089265038\n",
      "\tEval loss:  0.7462292243452633\n",
      "Epoch:  7\n",
      "\tTrain loss:  0.6394221372464124\n",
      "\tEval loss:  0.7418948341818417\n",
      "Epoch:  8\n",
      "\tTrain loss:  0.5864623855142033\n",
      "\tEval loss:  0.730551532086204\n",
      "Epoch:  9\n",
      "\tTrain loss:  0.5540207045919755\n",
      "\tEval loss:  0.7163246028563556\n"
     ]
    }
   ],
   "source": [
    "model = Classifier(len(train_langds.src_vocab), len(train_langds.trg_vocab), 125, 2)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    print('Epoch: ', epoch)\n",
    "    for idx, batch in enumerate(train_dt):\n",
    "        src = batch[\"src\"]  # shape --> e.g. (19, 2) sentence len, batch size\n",
    "        trg = batch[\"trg\"]  # shape --> e.g. (3, 2) sentence len, batch size\n",
    "\n",
    "        # Clear the accumulating gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(src)\n",
    "\n",
    "        # Calculate the loss value for every epoch\n",
    "        loss = criterion(output.squeeze(0), trg.squeeze(0))\n",
    "\n",
    "        # Calculate the gradients for weights & biases using back-propagation\n",
    "        loss.backward()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # Clip the gradient value is it exceeds > 1\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "\n",
    "        # Update the weights values\n",
    "        optimizer.step()\n",
    "    print('\\tTrain loss: ', epoch_loss/len(train_dt))\n",
    "    \n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(valid_dt):\n",
    "            src = batch[\"src\"]  # shape --> e.g. (19, 2) sentence len, batch size\n",
    "            trg = batch[\"trg\"]  # shape --> e.g. (3, 2) sentence len, batch size\n",
    "\n",
    "            output = model(src)\n",
    "\n",
    "            # Calculate the loss value for every epoch\n",
    "            loss = criterion(output.squeeze(0), trg.squeeze(0))\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "    \n",
    "    print('\\tEval loss: ', epoch_loss/len(valid_dt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6988416988416989\n"
     ]
    }
   ],
   "source": [
    "total = len(valid_ds)\n",
    "correct = 0\n",
    "\n",
    "for i in valid_langds:\n",
    "    inp = torch.tensor(i['src']).unsqueeze(1)\n",
    "    trg = i['trg'][0]\n",
    "    \n",
    "    pred = model(inp).view(-1).argmax().item()\n",
    "    \n",
    "    if pred == trg:\n",
    "        correct += 1\n",
    "\n",
    "print('Accuracy: ', correct/total)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
