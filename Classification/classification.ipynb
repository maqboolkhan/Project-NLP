{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "We are going to explore and study a basic and important task of Natural Language Processiing (NLP) which is text classification. This task can help us with various tasks for example sentiments analysis, langauge detection, text author detection and text labelling are only few to name. In this Notebook, we will implement text's langauge detection task using two NLP approaches. First approach is with statistics and we will developed very famous Naive Bayes classifier. Second approach is with Machine Learning and there we will implement a very simple single layer Long Short Term Memory (LSTM) model. We will use Accuracy to compare both models performance (we could also use F1-measure but I want to keep things simple). I used vanilla Python for Naive Bayes implementation and Pytorch for LSTM. The dataset I use is from [Kaggle](https://www.kaggle.com/basilb2s/language-detection) and it consist of text with their respective langauge label. The dataset contains text from 17 different languages, however I picked only 3 langauges with the most enteries (English, French and Spanish). I used 80% data for training and remaining 20% for validation.\n",
    "Any classification task can be further divided into binary or multi-class classification. Here as we picked 3 langauges so we are going to develop multi class classifier. Let's now start with Naive Bayes implementation.\n",
    "\n",
    "### Naive Bayes\n",
    "\n",
    "Naive Bayes is one of the important statistical algorithm for text classification. It has two assumptions:\n",
    "\n",
    "1. _Bag-of-words assumptions:_ Position of words in text does not matter and all the words are equally important. \n",
    "2. _Conditional Independence_: Features (words) probabilities $P(w|c)$ are independent of each other given class c.\n",
    "\n",
    "Naive Bayes is **naive** because of the **first** assumption. This is a strong assumption and **unrealistic for real data; however, this is very effective**. One thing two remember that both assumptions are incorrect, but they simplify the algorithm big time.\n",
    "\n",
    "Some notes from lectures of Dan Jurafsky (highly recommend watching [these](https://www.youtube.com/playlist?list=PLLssT5z_DsK8HbD2sPcUIDfQ7zmBarMYv) videos from him):\n",
    "- Small data ? Naive bayes is perfect\n",
    "- Reasonable amount of data? go with SVM or Logistic Regression\n",
    "- Huge data? Training ML models can be time consuming and here Naive Bayes will shine. Infact with enough data classifier wont even matter!\n",
    "\n",
    "Before diving into more details for Naive Bayes algorithm, lets analyze and prepare the dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 languages in dataset \n",
      "Total dataset count: 10337\n",
      "{'English': 1385, 'Malayalam': 594, 'Hindi': 63, 'Tamil': 469, 'Portugeese': 739, 'French': 1014, 'Dutch': 546, 'Spanish': 819, 'Greek': 365, 'Russian': 692, 'Danish': 428, 'Italian': 698, 'Turkish': 474, 'Sweedish': 676, 'Arabic': 536, 'German': 470, 'Kannada': 369}\n",
      "Total dataset count (3 languages): 3218\n",
      "\n",
      "\n",
      "Train set: \n",
      "\tTotat: 2700 \n",
      "\tEnglish: 1169, French: 843 Spanish: 688\n",
      "Train set: \n",
      "\tTotat: 518 \n",
      "\tEnglish: 216, French: 171 Spanish: 131\n"
     ]
    }
   ],
   "source": [
    "# dataset courtesy: https://www.kaggle.com/basilb2s/language-detection\n",
    "ds = pd.read_csv('Language Detection.csv')\n",
    "\n",
    "print(f'{len(ds[\"Language\"].unique())} languages in dataset ')\n",
    "print(f'Total dataset count: {len(ds)}')\n",
    "\n",
    "stats = { lang: len(ds.loc[ds['Language'] == lang])  for lang in ds[\"Language\"].unique()  }\n",
    "print(stats)\n",
    "\n",
    "np.random.shuffle(ds.values)\n",
    "total_ds = ds.loc[(ds['Language'] == 'Spanish') | (ds['Language'] == 'French') | (ds['Language'] == 'English')]\n",
    "print(f'Total dataset count (3 languages): {len(total_ds)}')\n",
    "\n",
    "train_ds = total_ds[:2700]\n",
    "valid_ds = total_ds[2700:]\n",
    "\n",
    "# Only considering three languages en, fr and es\n",
    "ds_en = train_ds.loc[train_ds['Language'] == 'English']\n",
    "ds_fr = train_ds.loc[train_ds['Language'] == 'French']\n",
    "ds_es = train_ds.loc[train_ds['Language'] == 'Spanish']\n",
    "\n",
    "vds_en = valid_ds.loc[valid_ds['Language'] == 'English']\n",
    "vds_fr = valid_ds.loc[valid_ds['Language'] == 'French']\n",
    "vds_es = valid_ds.loc[valid_ds['Language'] == 'Spanish']\n",
    "\n",
    "print('\\n')\n",
    "print(f'Train set: \\n\\tTotat: {len(train_ds)} \\n\\tEnglish: {len(ds_en)}, French: {len(ds_fr)} Spanish: {len(ds_es)}')\n",
    "print(f'Train set: \\n\\tTotat: {len(valid_ds)} \\n\\tEnglish: {len(vds_en)}, French: {len(vds_fr)} Spanish: {len(vds_es)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenating all rows into one corpus\n",
    "corpus_en = ds_en.Text.str.cat()\n",
    "corpus_fr = ds_fr.Text.str.cat()\n",
    "corpus_es = ds_es.Text.str.cat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have dataset for all three classes. Let's talk about our algorithm. So we want to find out probability of class _c_ given the document _d_ hence $P(c|d)$. According to the Bayes rule we can calculate it as\n",
    "\n",
    "$$ P(c|d) = \\frac{P(d|c)P(c)}{P(d)} $$\n",
    "\n",
    "Where $P(d)$ is the probability of the given document which is always constant (as we have fixed dataset) so we can drop it. Now it becomes\n",
    "\n",
    "$$ P(c|d) = P(d|c)P(c) $$\n",
    "\n",
    "Now $P(c)$ is probability of the class and we also call it **prior probability**. Lets calculate it first!\n",
    "\n",
    "\n",
    "$$ P(c) = \\frac{\\text{No. of c documents}}{\\text{Total no. of documents}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of en docs 1173 P(en): 0.43444444444444447\n",
      "No. of fr docs 840 P(fr): 0.3111111111111111\n",
      "No. of es docs 687 P(es): 0.2544444444444444\n"
     ]
    }
   ],
   "source": [
    "# Calculating prior probabilities\n",
    "prior_en = len(ds_en)/no_of_docs\n",
    "print(f\"No. of en docs {len(ds_en)} P(en): {prior_en}\")\n",
    "\n",
    "prior_fr = len(ds_fr)/no_of_docs\n",
    "print(f\"No. of fr docs {len(ds_fr)} P(fr): {prior_fr}\")\n",
    "\n",
    "prior_es = len(ds_es)/no_of_docs\n",
    "print(f\"No. of es docs {len(ds_es)} P(es): {prior_es}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our all three prior probabilities calculated. Its time to deal with $P(d|c)$ which we call **likelihood**. As _d_ is a document consists of many words _w_ we can rewrite as \n",
    "\n",
    "$$ P(c|d) = P(w_1 .... w_n|c)P(c) $$\n",
    "\n",
    "Now calculating $P(w_1 .... w_n|c)$ is not trivial, but because of the assumption of __*independence*__, joint probability will become\n",
    "\n",
    "\n",
    "$$ P(c|d) = P(w_1|c) P(w_2|c) ... P(w_n|c) P(c) $$\n",
    "\n",
    "Now the only calculation we need to do is for $P(w_i|c)$ and it can be calculated as\n",
    "\n",
    "$$P(w_i|c) = \\frac{ \\text{ No. of $w_i$ in c + 1} }{ \\text{No. of words in c + (Vocab size + 1)} } $$\n",
    "\n",
    "Here the _Vocab size_ is the number of unique words in all three classes (the complete corpus). We added + 1 in both the nominator and denominator to handle unknown words!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24519\n",
      "18629\n",
      "13984\n"
     ]
    }
   ],
   "source": [
    "# Pre-processing\n",
    "# Lower casing and tokenization i.e here we are simply splitting by spaces\n",
    "tokens_en = corpus_en.lower().split()\n",
    "tokens_fr = corpus_fr.lower().split()\n",
    "tokens_es = corpus_es.lower().split()\n",
    "print(len(tokens_en))\n",
    "print(len(tokens_fr))\n",
    "print(len(tokens_es))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size:  16926\n"
     ]
    }
   ],
   "source": [
    "# Creating vacabs\n",
    "vocab_en = Counter()\n",
    "vocab_fr = Counter()\n",
    "vocab_es = Counter()\n",
    "\n",
    "vocab_en.update(tokens_en)\n",
    "vocab_fr.update(tokens_fr)\n",
    "vocab_es.update(tokens_es)\n",
    "\n",
    "size_vocab_en = len(vocab_en) \n",
    "size_vocab_fr = len(vocab_fr) \n",
    "size_vocab_es = len(vocab_es) \n",
    "\n",
    "\n",
    "# plus one for unknown word\n",
    "vocab_size = len(set(tokens_en)) + len(set(tokens_fr)) + len(set(tokens_es)) + 1\n",
    "\n",
    "print('Vocab size: ', vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nb(text, cls_w_count, cls_prior, cls_vocab, vocab_size):\n",
    "    tokens = text.strip().lower().split()\n",
    "    res = cls_prior\n",
    "    deno = cls_w_count + vocab_size\n",
    "    \n",
    "    for t in tokens:\n",
    "        nom = cls_vocab.get(t, 0) + 1\n",
    "        res *= (nom / deno)\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     English       0.00      0.00      0.00       212\n",
      "      French       0.00      0.00      0.00       174\n",
      "     Spanish       0.25      1.00      0.41       132\n",
      "\n",
      "    accuracy                           0.25       518\n",
      "   macro avg       0.08      0.33      0.14       518\n",
      "weighted avg       0.06      0.25      0.10       518\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniconda/base/envs/p_nlp/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/Caskroom/miniconda/base/envs/p_nlp/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/Caskroom/miniconda/base/envs/p_nlp/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "langs = {\n",
    "        'English': {'prior': prior_en, 'tokens': len(tokens_en), 'vocab': vocab_en },\n",
    "        'French': {'prior': prior_fr, 'tokens': len(tokens_fr), 'vocab': vocab_fr },\n",
    "        'Spanish': {'prior': prior_es, 'tokens': len(tokens_es), 'vocab': vocab_es },\n",
    "    }\n",
    "\n",
    "last_pred = -1\n",
    "\n",
    "true_labels =[]\n",
    "pred_labels =[]\n",
    "\n",
    "for i, row in valid_ds.iterrows():\n",
    "    text = row.Text\n",
    "    label = row.Language\n",
    "    true_labels.append(label)\n",
    "    for lang in langs.keys():\n",
    "        lang_obj = langs.get(lang)\n",
    "        prior = lang_obj.get('prior')\n",
    "        pred = nb(text, lang_obj.get('tokens'), prior, lang_obj.get('vocab'), vocab_size)\n",
    "        if pred > last_pred:\n",
    "            lp = pred\n",
    "            pred_lang = lang\n",
    "    pred_labels.append(pred_lang)\n",
    "\n",
    "print(classification_report(true_labels, pred_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As our dataset is pretty imbalanced we have to consider `macro avg`. So with Naive Bayes we were only able to achieve 0.14 F1-score on our validation set and also it was unable to correctly classify English and French (hence, Sckit-learn giving us warning). But as you have noticed it took almost no time to train and generate predictions. Hence, quite fast!\n",
    "Now lets see the same task with Deep Learning approach!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning Approach\n",
    "\n",
    "So for Deep Learning we will use LSTM. I would not explain all the technicalities invovled in LSTM and deep learning (e.g batch, optimizer etc), rather I would discuss how to implement LSTM with Pytorch and how Deep Learning can help us to improve classification performance!\n",
    "However, I would highly recommend very easily explained article from Olah _[Understanding LSTM](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)_. \n",
    "Lets import Pytorch's modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp_params = {\n",
    "    \"batch_size\": 32,\n",
    "    \"embedding_dim\": 125,\n",
    "    \"hidden_dim\": 2\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as always, first step is to prepare data. We would create a parallel (source and target) kind of dataset where source is the text and target is the langauge of the source text. In Pytorch, we have to create a `Class` for dataset which should be inhereted from `Dataset` class provided by Pytorch and it must implement three functions: `__init__, __len__, and __getitem__`. `__len__` should return number of rows in our datasets and `__getitem__` should return a pair of `source` and `target`. We will also generate vocabulary of our dataset and encode our text and language using this vocabulary. Now let's code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LangDataset(Dataset):\n",
    "    def __init__(self, ds, train_vocab=None):\n",
    "        self.corpus = ds\n",
    "\n",
    "        if not train_vocab:\n",
    "            self.src_vocab, self.trg_vocab = self._build_vocab()\n",
    "        else:\n",
    "            self.src_vocab, self.trg_vocab = train_vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.corpus)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        text = self.corpus.iloc[item].Text\n",
    "        lang = self.corpus.iloc[item].Language\n",
    "        \n",
    "        return {\n",
    "            'src': self.src_vocab.lookup_indices(text.lower().split()),\n",
    "            'trg': self.trg_vocab.lookup_indices([lang])\n",
    "        }\n",
    "    \n",
    "    def _build_vocab(self):\n",
    "        src_tokens = self.corpus.Text.str.cat().lower().split()\n",
    "        \n",
    "        src_vocab = build_vocab_from_iterator([src_tokens], specials=[\"<unk>\",\"<pad>\"])\n",
    "        src_vocab.set_default_index(src_vocab['<unk>'])\n",
    "        \n",
    "        trg_vocab = build_vocab_from_iterator([['English', 'French', 'Spanish']])\n",
    "\n",
    "        return src_vocab, trg_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we only need to make batches then we would ready for model's training. To ease creation of batches, Pytorch provides really handy `DataLoader` class. We need to instantiate this class and provide `dataset` class, batch size and a collate function. So now what is collate function?\n",
    "Collate function is a way to customize data batch process and shuffling. For this tutorial, we could avoid it and let Pytorch use its default collate function but learning it comes very handy in complex situation. So here we are just creating batch ourself, putting tensors on device and also padding them so that they are equal in length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch, pad_value, device):\n",
    "    trgs = []\n",
    "    srcs = []\n",
    "    for row in batch:\n",
    "        srcs.append(torch.tensor(row[\"src\"], dtype=torch.long).to(device))\n",
    "        trgs.append(torch.tensor(row[\"trg\"]).to(device))\n",
    "\n",
    "    padded_srcs = pad_sequence(srcs, padding_value=pad_value)\n",
    "    padded_trgs = pad_sequence(trgs, padding_value=pad_value)\n",
    "    return {\"src\": padded_srcs, \"trg\": padded_trgs}\n",
    "    \n",
    "train_langds = LangDataset(train_ds)\n",
    "valid_langds = LangDataset(valid_ds, (train_langds.src_vocab, train_langds.trg_vocab))\n",
    "\n",
    "pad_value = train_langds.src_vocab['<pad>']\n",
    "\n",
    "train_dt = DataLoader(train_langds, batch_size=hyp_params[\"batch_size\"], shuffle=\n",
    "                   True, collate_fn=lambda batch_size: collate_fn(batch_size, pad_value, device))\n",
    "\n",
    "valid_dt = DataLoader(valid_langds, batch_size=hyp_params[\"batch_size\"], shuffle=\n",
    "                   True, collate_fn=lambda batch_size: collate_fn(batch_size, pad_value, device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, its time to talk about our classification model. Its really simple its has a embedding layer, then LSTM layer and at the end a linear layer. Embedding layer takes our tensors and return word embedding for them. Then we feed these embeddings to LSTM and finally we send hidden state of LSTM to a linear layer!\n",
    "\n",
    "The question we should ask here is why we are not using output of LSTM for predictions? (I would try my best to not to go in theoritical details and explain clearly) Actually LSTM is a recurrent neural network and these network are designed to process sequential data. So LSTM takes each word at a time and produce output. So we have output for every input in the output of LSTM which we definitly we dont want. Rather, we want output after the complete sentence. So in `hidden_state` we have result of last token and this result was calculated using all the previous tokens as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim):\n",
    "        super(Classifier, self).__init__()\n",
    "\n",
    "\n",
    "        # Embedding is just an lookup table of size \"vocab_size\"\n",
    "        # and each element has \"embedding_size\" dimension\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.LSTM = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Shape --> [Sequence_length , batch_size , embedding dims]\n",
    "        embedding = self.embedding(x)\n",
    "        # Shape --> (output) [Sequence_length , batch_size , hidden_size]\n",
    "        # Shape --> (hs, cs) [num_layers, batch_size size, hidden_size]\n",
    "        outputs, (hidden_state, cell_state) = self.LSTM(embedding)\n",
    "        \n",
    "        linear_outputs = self.fc(hidden_state)\n",
    "        \n",
    "        return linear_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters\n",
    "Before going to the training details, I would like to mention that `batch size`, `embedding dim` and `hidden dim` are hypermeters. One can experiment with different values of them and may find better accuracy (fine-tuning).\n",
    "\n",
    "As we are doing multi-class classification, so we will use `CrossEntropyLoss`. Now lets train our model for just 10 epochs and see: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "\tTrain loss:  1.1044260284479928 Eval loss:  1.0990194082260132\n",
      "Epoch:  1\n",
      "\tTrain loss:  1.0645176929586073 Eval loss:  1.0432960145613726\n",
      "Epoch:  2\n",
      "\tTrain loss:  1.0041135640705332 Eval loss:  0.9902160132632536\n",
      "Epoch:  3\n",
      "\tTrain loss:  0.9759966948453118 Eval loss:  0.9441788371871499\n",
      "Epoch:  4\n",
      "\tTrain loss:  0.9045599348404828 Eval loss:  0.8792884034268996\n",
      "Epoch:  5\n",
      "\tTrain loss:  0.8536732028512394 Eval loss:  0.8448967968716341\n",
      "Epoch:  6\n",
      "\tTrain loss:  0.8074105367941015 Eval loss:  0.8013440615990582\n",
      "Epoch:  7\n",
      "\tTrain loss:  0.7658839534310734 Eval loss:  0.7683879137039185\n",
      "Epoch:  8\n",
      "\tTrain loss:  0.7182312299223507 Eval loss:  0.7353206031462726\n",
      "Epoch:  9\n",
      "\tTrain loss:  0.6748375633183648 Eval loss:  0.6881032936713275\n"
     ]
    }
   ],
   "source": [
    "model = Classifier(len(train_langds.src_vocab), len(train_langds.trg_vocab), hyp_params[\"embedding_dim\"], hyp_params[\"hidden_dim\"])\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    print('Epoch: ', epoch)\n",
    "    for idx, batch in enumerate(train_dt):\n",
    "        src = batch[\"src\"]  # shape --> e.g. (19, 2) sentence len, batch size\n",
    "        trg = batch[\"trg\"]  # shape --> e.g. (3, 2) sentence len, batch size\n",
    "\n",
    "        # Clear the accumulating gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # shape --> (1, 32, 3) 1, batch size, trg vocab\n",
    "        output = model(src)\n",
    "\n",
    "        # Calculate the loss value for every epoch\n",
    "        # Squeezing to remove first dimension \n",
    "        loss = criterion(output.squeeze(0), trg.squeeze(0))\n",
    "\n",
    "        # Calculate the gradients for weights & biases using back-propagation\n",
    "        loss.backward()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # Clip the gradient value is it exceeds > 1\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "\n",
    "        # Update the weights values\n",
    "        optimizer.step()\n",
    "    print('\\tTrain loss: ', epoch_loss/len(train_dt), end=\" \")\n",
    "    \n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(valid_dt):\n",
    "            src = batch[\"src\"]  # shape --> e.g. (19, 2) sentence len, batch size\n",
    "            trg = batch[\"trg\"]  # shape --> e.g. (3, 2) sentence len, batch size\n",
    "\n",
    "            output = model(src)\n",
    "\n",
    "            # Calculate the loss value for every epoch\n",
    "            loss = criterion(output.squeeze(0), trg.squeeze(0))\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "    \n",
    "    print('Eval loss: ', epoch_loss/len(valid_dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Classifier(\n",
       "  (embedding): Embedding(16256, 125)\n",
       "  (LSTM): LSTM(125, 2)\n",
       "  (fc): Linear(in_features=2, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.94      0.87       212\n",
      "           1       0.70      0.94      0.80       174\n",
      "           2       0.60      0.18      0.28       132\n",
      "\n",
      "    accuracy                           0.75       518\n",
      "   macro avg       0.70      0.69      0.65       518\n",
      "weighted avg       0.72      0.75      0.70       518\n",
      "\n"
     ]
    }
   ],
   "source": [
    "true_labels =[]\n",
    "pred_labels =[]\n",
    "\n",
    "for i in valid_langds:\n",
    "    inp = torch.tensor(i['src']).unsqueeze(1)\n",
    "    trg = i['trg'][0]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pred = model(inp).view(-1).argmax().item()\n",
    " \n",
    "    true_labels.append(trg)\n",
    "    pred_labels.append(pred)\n",
    "\n",
    "print(classification_report(true_labels, pred_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baaam, F1-score of 65 which is 4 times more than Naive Bayes. However, it is bit more difficult to code a LSTM model and it takes time to train, but just notice how drastically deep learning improved our model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'English'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_map = {0: 'English', 1: 'French', 2: 'Spanish'}\n",
    "\n",
    "text = \"hello, how are you?\"\n",
    "txt_to_ind = train_langds.src_vocab.lookup_indices(text.split())\n",
    "inp_tensor = torch.tensor(txt_to_ind).unsqueeze(1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    res = model(inp_tensor).view(-1).argmax().item()\n",
    "out_map[res]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
