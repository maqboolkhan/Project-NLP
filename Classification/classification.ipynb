{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UQhmNKRTPBrM"
   },
   "source": [
    "# Classification\n",
    "\n",
    "We are going to explore and study a basic and important task of Natural Language Processiing (NLP) which is text classification. This task can help us with various tasks for example sentiments analysis, langauge detection, text author detection and text labelling are only few to name. In this Notebook, we will implement text's langauge detection task using two NLP approaches. First approach is with statistics and we will developed very famous Naive Bayes classifier. Second approach is with Machine Learning and there we will implement a very simple single layer Long Short Term Memory (LSTM) model. We will use Accuracy to compare both models performance (we could also use F1-measure but I want to keep things simple). I used vanilla Python for Naive Bayes implementation and Pytorch for LSTM. The dataset I use is from [Kaggle](https://www.kaggle.com/basilb2s/language-detection) and it consist of text with their respective langauge label. The dataset contains text from 17 different languages, however I picked only 3 langauges with the most enteries (English, French and Spanish). I used 80% data for training and remaining 20% for validation.\n",
    "Any classification task can be further divided into binary or multi-class classification. Here as we picked 3 langauges so we are going to develop multi class classifier. Let's now start with Naive Bayes implementation.\n",
    "\n",
    "### Naive Bayes\n",
    "\n",
    "Naive Bayes is one of the important statistical algorithm for text classification. It has two assumptions:\n",
    "\n",
    "1. _Bag-of-words assumptions:_ Position of words in text does not matter and all the words are equally important. \n",
    "2. _Conditional Independence_: Features (words) probabilities $P(w|c)$ are independent of each other given class c.\n",
    "\n",
    "Naive Bayes is **naive** because of the **first** assumption. This is a strong assumption and **unrealistic for real data; however, this is very effective**. One thing two remember that both assumptions are incorrect, but they simplify the algorithm big time.\n",
    "\n",
    "Some notes from lectures of Dan Jurafsky (highly recommend watching [these](https://www.youtube.com/playlist?list=PLLssT5z_DsK8HbD2sPcUIDfQ7zmBarMYv) videos from him):\n",
    "- Small data ? Naive bayes is perfect\n",
    "- Reasonable amount of data? go with SVM or Logistic Regression\n",
    "- Huge data? Training ML models can be time consuming and here Naive Bayes will shine. Infact with enough data classifier wont even matter!\n",
    "\n",
    "Before diving into more details for Naive Bayes algorithm, lets analyze and prepare the dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "OfjccBfgPBrS"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "7bJrZnqiPQKL"
   },
   "outputs": [],
   "source": [
    "!wget -q https://raw.githubusercontent.com/maqboolkhan/Project-NLP/master/Classification/Language%20Detection.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258
    },
    "id": "bJqr0R-KPBrU",
    "outputId": "059bd240-c43b-4a19-f905-5538764a6cf5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 languages in dataset \n",
      "Total dataset count: 10337\n",
      "Total dataset using for this notebook count (3 languages): 3218\n",
      "Validation set: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-66a0113f-193e-4c01-8ab6-4eaa1b8d7751\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Language</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>English</th>\n",
       "      <td>1181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>French</th>\n",
       "      <td>848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Spanish</th>\n",
       "      <td>671</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-66a0113f-193e-4c01-8ab6-4eaa1b8d7751')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-66a0113f-193e-4c01-8ab6-4eaa1b8d7751 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-66a0113f-193e-4c01-8ab6-4eaa1b8d7751');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "          Text\n",
       "Language      \n",
       "English   1181\n",
       "French     848\n",
       "Spanish    671"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset courtesy: https://www.kaggle.com/basilb2s/language-detection\n",
    "ds = pd.read_csv('Language Detection.csv')\n",
    "\n",
    "print(f'{len(ds[\"Language\"].unique())} languages in dataset ')\n",
    "print(f'Total dataset count: {len(ds)}')\n",
    "\n",
    "np.random.shuffle(ds.values)\n",
    "\n",
    "# Only considering three languages en, fr and es\n",
    "total_ds = ds.loc[(ds['Language'] == 'Spanish') | (ds['Language'] == 'French') | (ds['Language'] == 'English')]\n",
    "print(f'Total dataset using for this notebook count (3 languages): {len(total_ds)}')\n",
    "\n",
    "no_of_docs = 2700\n",
    "train_ds = total_ds[:no_of_docs]\n",
    "valid_ds = total_ds[no_of_docs:]\n",
    "\n",
    "print(\"Validation set: \\n\")\n",
    "train_ds.groupby('Language').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "id": "ZZivD66wPBrV",
    "outputId": "a15d6161-84a0-4219-8966-c6e594a5af7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-853bbe22-8cb3-4e36-8bb8-105290c8cc38\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Language</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>English</th>\n",
       "      <td>204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>French</th>\n",
       "      <td>166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Spanish</th>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-853bbe22-8cb3-4e36-8bb8-105290c8cc38')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-853bbe22-8cb3-4e36-8bb8-105290c8cc38 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-853bbe22-8cb3-4e36-8bb8-105290c8cc38');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "          Text\n",
       "Language      \n",
       "English    204\n",
       "French     166\n",
       "Spanish    148"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Validation set: \\n\")\n",
    "valid_ds.groupby('Language').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "btCQ7ImYPBrW"
   },
   "outputs": [],
   "source": [
    "ds_en = train_ds.loc[train_ds['Language'] == 'English']\n",
    "ds_fr = train_ds.loc[train_ds['Language'] == 'French']\n",
    "ds_es = train_ds.loc[train_ds['Language'] == 'Spanish']\n",
    "\n",
    "# Concatenating all rows into one corpus\n",
    "corpus_en = ds_en.Text.str.cat(sep=' ')\n",
    "corpus_fr = ds_fr.Text.str.cat(sep=' ')\n",
    "corpus_es = ds_es.Text.str.cat(sep=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "czZmyVmwPBrW"
   },
   "source": [
    "Now we have dataset for all three classes. I would like to mention here that I am not removing `stopwords` and also not doing `lemmatization` both techniques can help us in to reduce model size and in some cases increases the performance. It should be easy to incorporate both techniques. \n",
    "\n",
    "Let's talk about our algorithm. So we want to find out probability of class _c_ given the document _d_ hence $P(c|d)$. According to the Bayes rule we can calculate it as\n",
    "\n",
    "$$ P(c|d) = \\frac{P(d|c)P(c)}{P(d)} $$\n",
    "\n",
    "Where $P(d)$ is the probability of the given document which is always constant (as we have fixed dataset) so we can drop it. Now it becomes\n",
    "\n",
    "$$ P(c|d) = P(d|c)P(c) $$\n",
    "\n",
    "Now $P(c)$ is probability of the class and we also call it **prior probability**. Lets calculate it first!\n",
    "\n",
    "\n",
    "$$ P(c) = \\frac{\\text{No. of c documents}}{\\text{Total no. of documents}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YXHYdaRwPBrX",
    "outputId": "36462283-8951-401f-b56b-303f17197a3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of en docs 1181 P(en): 0.4374074074074074\n",
      "No. of fr docs 848 P(fr): 0.31407407407407406\n",
      "No. of es docs 671 P(es): 0.24851851851851853\n"
     ]
    }
   ],
   "source": [
    "# Calculating prior probabilities\n",
    "prior_en = len(ds_en)/no_of_docs\n",
    "print(f\"No. of en docs {len(ds_en)} P(en): {prior_en}\")\n",
    "\n",
    "prior_fr = len(ds_fr)/no_of_docs\n",
    "print(f\"No. of fr docs {len(ds_fr)} P(fr): {prior_fr}\")\n",
    "\n",
    "prior_es = len(ds_es)/no_of_docs\n",
    "print(f\"No. of es docs {len(ds_es)} P(es): {prior_es}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EmgFcO78PBrY"
   },
   "source": [
    "Now we have our all three prior probabilities calculated. Its time to deal with $P(d|c)$ which we call **likelihood**. As _d_ is a document consists of many words _w_ we can rewrite as \n",
    "\n",
    "$$ P(c|d) = P(w_1 .... w_n|c)P(c) $$\n",
    "\n",
    "Now calculating $P(w_1 .... w_n|c)$ is not trivial, but because of the assumption of __*independence*__, joint probability will become\n",
    "\n",
    "\n",
    "$$ P(c|d) = P(w_1|c) P(w_2|c) ... P(w_n|c) P(c) $$\n",
    "\n",
    "Now the only calculation we need to do is for $P(w_i|c)$ and it can be calculated as\n",
    "\n",
    "$$P(w_i|c) = \\frac{ \\text{ No. of $w_i$ in c + 1} }{ \\text{No. of words in c + Vocab size} } $$\n",
    "\n",
    "Here the _Vocab size_ is the number of unique words in all three classes (the complete corpus). We added + 1 in the nominator and denominator to handle unknown words (It's called Laplace smoothing)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yYC0MGMnPBrY",
    "outputId": "26794544-7e0b-416d-8573-5d06a0836ac1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25988\n",
      "19415\n",
      "13805\n"
     ]
    }
   ],
   "source": [
    "# Pre-processing\n",
    "# Lower casing and tokenization i.e here we are simply splitting by spaces\n",
    "tokens_en = corpus_en.lower().split()\n",
    "tokens_fr = corpus_fr.lower().split()\n",
    "tokens_es = corpus_es.lower().split()\n",
    "print(len(tokens_en))\n",
    "print(len(tokens_fr))\n",
    "print(len(tokens_es))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ab2I9R46PBrZ",
    "outputId": "448472f7-fa68-4863-c315-9e80dd3f2b76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size:  16756\n"
     ]
    }
   ],
   "source": [
    "# Creating vacabs\n",
    "vocab_en = Counter()\n",
    "vocab_fr = Counter()\n",
    "vocab_es = Counter()\n",
    "\n",
    "vocab_en.update(tokens_en)\n",
    "vocab_fr.update(tokens_fr)\n",
    "vocab_es.update(tokens_es)\n",
    "\n",
    "size_vocab_en = len(vocab_en)\n",
    "size_vocab_fr = len(vocab_fr)\n",
    "size_vocab_es = len(vocab_es)\n",
    "\n",
    "vocab_size = len(set(tokens_en)) + len(set(tokens_fr)) + len(set(tokens_es))\n",
    "\n",
    "print('Vocab size: ', vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "HhKWE2pCPBrZ"
   },
   "outputs": [],
   "source": [
    "def nb(text, nb_component, vocab_size):\n",
    "    tokens = text.strip().lower().split()\n",
    "    res = nb_component.get('prior')\n",
    "    deno = nb_component.get('no_tokens') + vocab_size\n",
    "    for t in tokens:\n",
    "        nom = nb_component.get('vocab').get(t, 0) + 1\n",
    "        t_pred = (nom / deno)\n",
    "        res *= t_pred\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "lV6ibYiVPBra"
   },
   "outputs": [],
   "source": [
    "# Required components of Naive Bayes\n",
    "nb_components = {\n",
    "    'English': {'prior': prior_en, 'no_tokens': len(tokens_en), 'vocab': vocab_en },\n",
    "    'French': {'prior': prior_fr, 'no_tokens': len(tokens_fr), 'vocab': vocab_fr },\n",
    "    'Spanish': {'prior': prior_es, 'no_tokens': len(tokens_es), 'vocab': vocab_es },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B-r3fJS8PBra",
    "outputId": "b3fba756-4f50-471f-a3ce-b99405c2d316",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     English       0.99      1.00      0.99       204\n",
      "      French       1.00      0.98      0.99       166\n",
      "     Spanish       0.99      0.99      0.99       148\n",
      "\n",
      "    accuracy                           0.99       518\n",
      "   macro avg       0.99      0.99      0.99       518\n",
      "weighted avg       0.99      0.99      0.99       518\n",
      "\n"
     ]
    }
   ],
   "source": [
    "true_labels =[]\n",
    "pred_labels =[]\n",
    "\n",
    "for i, row in valid_ds.iterrows():\n",
    "    last_pred = -1\n",
    "    for lang in nb_components.keys():\n",
    "        nb_component = nb_components.get(lang)\n",
    "        pred = nb(row.Text, nb_component, vocab_size)\n",
    "\n",
    "        if pred > last_pred:\n",
    "            last_pred = pred\n",
    "            pred_lang = lang\n",
    "    true_labels.append(row.Language)\n",
    "    pred_labels.append(pred_lang)\n",
    "\n",
    "\n",
    "print(classification_report(true_labels, pred_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1b1TInLvPBrb"
   },
   "source": [
    "As our dataset is pretty imbalanced we have to consider `macro avg`. So with Naive Bayes, we are easily able to train robust classifier in no time! (Remember, here I did not to cross-validation and you should always do it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "itGADBm9PBrc"
   },
   "source": [
    "I know, I know it is very messy to implement Naive Bayes! \n",
    "> Basically, I tried to as explicit as possible with the implementation. So that we can see how we implement the theory! \n",
    "\n",
    "If you are interested here is `Scikit-learn` way!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BJ3DyWOoPBrc",
    "outputId": "6f60cc14-5327-4e4a-fc02-50c4256ddab4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99       204\n",
      "           1       0.99      0.98      0.99       166\n",
      "           2       1.00      0.99      1.00       148\n",
      "\n",
      "    accuracy                           0.99       518\n",
      "   macro avg       0.99      0.99      0.99       518\n",
      "weighted avg       0.99      0.99      0.99       518\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sklearn as sk\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "X_train = train_ds.Text\n",
    "Y_train = train_ds.Language\n",
    "\n",
    "vocab_train = CountVectorizer()\n",
    "X_train_encoded = vocab_train.fit_transform(X_train)\n",
    "Y_train_encoded = label_encoder.fit_transform(Y_train)\n",
    "\n",
    "nb_classifier = MultinomialNB().fit(X_train_encoded, Y_train_encoded)\n",
    "\n",
    "X_valid = valid_ds.Text\n",
    "Y_valid = valid_ds.Language\n",
    "\n",
    "X_valid_encoded = vocab_train.transform(X_valid)\n",
    "Y_valid_encoded = label_encoder.fit_transform(Y_valid)\n",
    "\n",
    "predicted = nb_classifier.predict(X_valid_encoded)\n",
    "\n",
    "print(classification_report(Y_valid_encoded, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YLoMRHNoPBrd"
   },
   "source": [
    "### Deep Learning Approach\n",
    "\n",
    "So for Deep Learning we will use LSTM. I would not explain all the technicalities invovled in LSTM and deep learning (e.g batch, optimizer etc), rather I would discuss how to implement LSTM with Pytorch and how Deep Learning can help us to improve classification performance!\n",
    "However, I would highly recommend very easily explained article from Olah _[Understanding LSTM](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)_. \n",
    "Lets import Pytorch's modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IGsN85aPPBrd",
    "outputId": "1f85f02e-c11f-4427-813e-c5a4daa2dd51"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Seeds for reproducible results\n",
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "j8V_cOy_PBre"
   },
   "outputs": [],
   "source": [
    "hyp_params = {\n",
    "    \"batch_size\": 64,\n",
    "    \"embedding_dim\": 125,\n",
    "    \"hidden_dim\": 10\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pwnsRbxIPBre"
   },
   "source": [
    "So as always, first step is to prepare data. We would create a parallel (source and target) kind of dataset where source is the text and target is the langauge of the source text. In Pytorch, we have to create a `Class` for dataset which should be inhereted from `Dataset` class provided by Pytorch and it must implement three functions: `__init__, __len__, and __getitem__`. `__len__` should return number of rows in our datasets and `__getitem__` should return a pair of `source` and `target`. We will also generate vocabulary of our dataset and encode our text and language using this vocabulary. Now let's code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "QcA0WILKPBrf"
   },
   "outputs": [],
   "source": [
    "class LangDataset(Dataset):\n",
    "    def __init__(self, ds, train_vocab=None):\n",
    "        self.corpus = ds\n",
    "\n",
    "        if not train_vocab:\n",
    "            self.src_vocab, self.trg_vocab = self._build_vocab()\n",
    "        else:\n",
    "            self.src_vocab, self.trg_vocab = train_vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.corpus)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        text = self.corpus.iloc[item].Text\n",
    "        lang = self.corpus.iloc[item].Language\n",
    "\n",
    "        return {\n",
    "            'src': self.src_vocab.lookup_indices(text.lower().split()),\n",
    "            'trg': self.trg_vocab.lookup_indices([lang])\n",
    "        }\n",
    "\n",
    "    def _build_vocab(self):\n",
    "        src_tokens = self.corpus.Text.str.cat().lower().split()\n",
    "\n",
    "        src_vocab = build_vocab_from_iterator([src_tokens], specials=[\"<unk>\",\"<pad>\"])\n",
    "        src_vocab.set_default_index(src_vocab['<unk>'])\n",
    "\n",
    "        trg_vocab = build_vocab_from_iterator([self.corpus.Language.unique().tolist()])\n",
    "\n",
    "        return src_vocab, trg_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vBDb_LTIPBrf"
   },
   "source": [
    "Now we only need to make batches then we would ready for model's training. To ease creation of batches, Pytorch provides really handy `DataLoader` class. We need to instantiate this class and provide `dataset` class, batch size and a collate function. So now what is collate function?\n",
    "Collate function is a way to customize data batch process and shuffling. For this tutorial, we could avoid it and let Pytorch use its default collate function but learning it comes very handy in complex situation. So here we are just creating batch ourself, putting tensors on device and also padding them so that they are equal in length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "erz2vbD6PBrg"
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch, pad_value, device):\n",
    "    trgs = []\n",
    "    srcs = []\n",
    "    for row in batch:\n",
    "        srcs.append(torch.tensor(row[\"src\"], dtype=torch.long).to(device))\n",
    "        trgs.append(torch.tensor(row[\"trg\"]).to(device))\n",
    "\n",
    "    padded_srcs = pad_sequence(srcs, padding_value=pad_value)\n",
    "    # We do not padding for targets because they are simply single classes\n",
    "    return {\"src\": padded_srcs, \"trg\": torch.tensor(trgs).to(device)}\n",
    "\n",
    "train_langds = LangDataset(train_ds)\n",
    "valid_langds = LangDataset(valid_ds, (train_langds.src_vocab, train_langds.trg_vocab))\n",
    "\n",
    "pad_value = train_langds.src_vocab['<pad>']\n",
    "\n",
    "train_dt = DataLoader(train_langds, batch_size=hyp_params[\"batch_size\"], shuffle=\n",
    "                   True, collate_fn=lambda batch_size: collate_fn(batch_size, pad_value, device))\n",
    "\n",
    "valid_dt = DataLoader(valid_langds, batch_size=hyp_params[\"batch_size\"], shuffle=\n",
    "                   True, collate_fn=lambda batch_size: collate_fn(batch_size, pad_value, device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a1ZB-ObnPBrg"
   },
   "source": [
    "Finally, its time to talk about our classification model. Its really simple its has a embedding layer, then LSTM layer and at the end a linear layer. Embedding layer takes our tensors and return word embedding for them. Then we feed these embeddings to LSTM and finally we send hidden state of LSTM to a linear layer!\n",
    "\n",
    "The question we should ask here is why we are not using output of LSTM for predictions? (I would try my best to not to go in theoritical details and explain clearly). Actually LSTM is a recurrent neural network and these network are designed to process sequential data. So LSTM takes each word at a time and produce output. So we have output for every input in the output of LSTM which we definitly we dont want. Rather, we want output after the complete sentence. So in `hidden_state` we have result of last token and this result was calculated using all the previous tokens as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "Lf9FNxP0PBrh"
   },
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        # Embedding is just an lookup table of size \"vocab_size\"\n",
    "        # and each element has \"embedding_size\" dimension\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.LSTM = nn.LSTM(embedding_dim, hidden_dim)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Shape --> [Sequence_length , batch_size , embedding dims]\n",
    "        embedding = self.embedding(x)\n",
    "        # Shape --> (output) [num_layers, batch_size, trg vocab]\n",
    "        # Shape --> (hs, cs) [num_layers, batch_size, hidden_size]\n",
    "        outputs, (hidden_state, cell_state) = self.LSTM(embedding)\n",
    "\n",
    "        linear_outputs = self.fc(hidden_state)\n",
    "\n",
    "        return linear_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uteQVH3jPBrh"
   },
   "source": [
    "#### Hyperparameters\n",
    "Before going to the training details, I would like to mention that `batch size`, `embedding dim` and `hidden dim` are hypermeters. One can experiment with different values of them and may find better accuracy (fine-tuning).\n",
    "\n",
    "As we are doing multi-class classification, so we will use `CrossEntropyLoss`. Now lets train our model for just 20 epochs and see: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F8PmqgWcPBrh",
    "outputId": "082e6a9d-38b8-48bb-f070-6dbb05e7ee5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "\tTrain loss:  1.0781073916790098 Eval loss:  1.0816357400682237\n",
      "Epoch:  1\n",
      "\tTrain loss:  1.070774113023004 Eval loss:  1.0860434903038874\n",
      "Epoch:  2\n",
      "\tTrain loss:  1.0713445863058402 Eval loss:  1.092643181482951\n",
      "Epoch:  3\n",
      "\tTrain loss:  1.070589778035186 Eval loss:  1.0907285081015692\n",
      "Epoch:  4\n",
      "\tTrain loss:  1.0699085329854212 Eval loss:  1.0847383207745023\n",
      "Epoch:  5\n",
      "\tTrain loss:  1.0674529671669006 Eval loss:  1.0974196725421481\n",
      "Epoch:  6\n",
      "\tTrain loss:  1.0694927337557771 Eval loss:  1.0935959153705173\n",
      "Epoch:  7\n",
      "\tTrain loss:  1.0692480469858923 Eval loss:  1.0935237540139093\n",
      "Epoch:  8\n",
      "\tTrain loss:  1.066483435242675 Eval loss:  1.0828029712041218\n",
      "Epoch:  9\n",
      "\tTrain loss:  1.066604338413061 Eval loss:  1.083164440260993\n",
      "Epoch:  10\n",
      "\tTrain loss:  1.0661731098973475 Eval loss:  1.107547230190701\n",
      "Epoch:  11\n",
      "\tTrain loss:  1.0637871567593065 Eval loss:  1.1002655161751642\n",
      "Epoch:  12\n",
      "\tTrain loss:  1.064213137293971 Eval loss:  1.087370236714681\n",
      "Epoch:  13\n",
      "\tTrain loss:  0.9705026274503663 Eval loss:  0.8440028230349222\n",
      "Epoch:  14\n",
      "\tTrain loss:  0.7086851707724637 Eval loss:  0.7425428695148892\n",
      "Epoch:  15\n",
      "\tTrain loss:  0.5440915150697841 Eval loss:  0.5169760452376472\n",
      "Epoch:  16\n",
      "\tTrain loss:  0.4206961906233499 Eval loss:  0.4602169593175252\n",
      "Epoch:  17\n",
      "\tTrain loss:  0.34454025258851606 Eval loss:  0.34173134134875405\n",
      "Epoch:  18\n",
      "\tTrain loss:  0.24549554288387299 Eval loss:  0.3137335363361571\n",
      "Epoch:  19\n",
      "\tTrain loss:  0.21280765758697376 Eval loss:  0.23516965988609526\n"
     ]
    }
   ],
   "source": [
    "model = Classifier(len(train_langds.src_vocab), len(train_langds.trg_vocab), hyp_params[\"embedding_dim\"], hyp_params[\"hidden_dim\"]).to(device)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(20):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    print('Epoch: ', epoch)\n",
    "    for idx, batch in enumerate(train_dt):\n",
    "\n",
    "        src = batch[\"src\"]  # shape --> e.g. (19, 2) sequence len, batch size\n",
    "        trg = batch[\"trg\"]  # shape --> e.g. (2) batch size\n",
    "\n",
    "        # Clear the accumulating gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # shape --> (1, 32, 3) 1, batch size, trg vocab\n",
    "        output = model(src)\n",
    "\n",
    "        # Calculate the loss value for every epoch\n",
    "        # Squeezing to remove first dimension \n",
    "        loss = criterion(output.squeeze(0), trg.squeeze(0))\n",
    "\n",
    "        # Calculate the gradients for weights & biases using back-propagation\n",
    "        loss.backward()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # Clip the gradient value is it exceeds > 1\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "\n",
    "        # Update the weights values\n",
    "        optimizer.step()\n",
    "    print('\\tTrain loss: ', epoch_loss/len(train_dt), end=\" \")\n",
    "\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(valid_dt):\n",
    "            src = batch[\"src\"]  # shape --> e.g. (19, 2) sentence len, batch size\n",
    "            trg = batch[\"trg\"]  # shape --> e.g. (2) batch size\n",
    "\n",
    "            output = model(src)\n",
    "\n",
    "            # Calculate the loss value for every epoch\n",
    "            loss = criterion(output.squeeze(0), trg.squeeze(0))\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    print('Eval loss: ', epoch_loss/len(valid_dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5bfiFUbUPBri",
    "outputId": "40af55d9-c6ad-4507-b656-6ebf01a98c95"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Classifier(\n",
       "  (embedding): Embedding(16063, 125)\n",
       "  (LSTM): LSTM(125, 10)\n",
       "  (fc): Linear(in_features=10, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8nPznM4nPBri",
    "outputId": "f06afa22-90b8-410f-a900-2733695a4eea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.93      0.95       204\n",
      "           1       0.97      0.92      0.94       166\n",
      "           2       0.83      0.93      0.88       148\n",
      "\n",
      "    accuracy                           0.92       518\n",
      "   macro avg       0.92      0.92      0.92       518\n",
      "weighted avg       0.93      0.92      0.92       518\n",
      "\n"
     ]
    }
   ],
   "source": [
    "true_labels =[]\n",
    "pred_labels =[]\n",
    "\n",
    "for i in valid_langds:\n",
    "    inp = torch.tensor(i['src']).unsqueeze(1).to(device)\n",
    "    trg = i['trg'][0]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred = model(inp).view(-1).argmax().item()\n",
    "\n",
    "    true_labels.append(trg)\n",
    "    pred_labels.append(pred)\n",
    "\n",
    "print(classification_report(true_labels, pred_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u7keuZINPBrj"
   },
   "source": [
    "It is clear that it is bit more difficult to code a LSTM model and it takes time to train, and in our case we only able to score F1-score of 92. However, with more epochs and hyperparameter tunning LSTM can be very robust classifier.\n",
    "Also, we have simple classification problem at hand, LSTM can out-class Naive Bayes on many different classification problems. However, we can not underestimate the power of simple and fast Naive Bayes!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hpkQMis0PBrj"
   },
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 37
    },
    "id": "CzH5B_ZvPBrj",
    "outputId": "8c48a2f4-3860-4356-dd7e-e633f9b31d75"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'English'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"hello, how are you?\"\n",
    "txt_to_ind = train_langds.src_vocab.lookup_indices(text.split())\n",
    "inp_tensor = torch.tensor(txt_to_ind).unsqueeze(1).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    res = model(inp_tensor).view(-1).argmax().item()\n",
    "\n",
    "train_langds.trg_vocab.lookup_token(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i8qc5NysRS9Y"
   },
   "source": [
    "So that's it!\n",
    "\n",
    "Foot note:\n",
    "If you are interested in `Transformer` model, check out this [repository](https://github.com/maqboolkhan/Transformer_classifier_pytorch)."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
