{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "567156ca-c204-40fd-b444-5324e9ac0570",
   "metadata": {},
   "source": [
    "# Language Models\n",
    "\n",
    "One of the important and almost backbone of different NLP related tasks!\n",
    "\n",
    "`# TODO: Add more details`\n",
    "\n",
    "We will explore statistical and machine learning methods!\n",
    "Lets starts with statistical methods first!\n",
    "\n",
    "### Statistical method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "878eaccc-0046-4566-b4a7-548ef75bf74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thanks https://nlpforhackers.io/language-models/\n",
    "import random\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "from nltk import bigrams, trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba8f499-decb-47b4-84c2-c8096784a3d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/maqboolkhan/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('reuters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13f198b0-0825-4d14-a389-caea81d5ada1",
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_grams = Counter(reuters.words())\n",
    "total_count = len(reuters.words())\n",
    " \n",
    "\n",
    "# Compute the probabilities (uni-grams)\n",
    "for word in uni_grams:\n",
    "    uni_grams[word] /= float(total_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "125f31ba-f033-409f-b112-81b5660042d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('.', 0.055021758950689205),\n",
       " (',', 0.042047741270415905),\n",
       " ('the', 0.033849129031826936),\n",
       " ('of', 0.02090707135390124),\n",
       " ('to', 0.01977743054365126),\n",
       " ('in', 0.015386126221089999),\n",
       " ('said', 0.014657438167564549),\n",
       " ('and', 0.014552260705293332),\n",
       " ('a', 0.013650988639090802),\n",
       " ('mln', 0.010481137497159917)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uni_grams_counter = Counter(uni_grams)\n",
    "uni_grams_counter.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdaaae21-afdc-4ef3-bcfc-0ab95347eb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(grams, n, context, length):\n",
    "    text = list(context)\n",
    "    context = context[0] if n == 2 else context # bigrams had , in tne context tuple hence to remedy that comma!\n",
    "    \n",
    "    for i in range(length):\n",
    "        sum = 0\n",
    "        \n",
    "        r = random.random()\n",
    "        \n",
    "        if context:\n",
    "            candidates = grams[context]\n",
    "        else:\n",
    "            candidates = grams\n",
    "        \n",
    "    \n",
    "        for k in candidates.keys():\n",
    "            sum += candidates[k]\n",
    "            if sum > r:\n",
    "                text.append(k)\n",
    "                \n",
    "                if context:\n",
    "                    context = (k) if n == 2 else (context[2-n], k)\n",
    "                \n",
    "                break\n",
    "    text = ['None' if token == None else token  for token in text] # Replacing None with 'None'\n",
    "    return ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "id": "85755ffb-ba9c-4208-ba04-4c2e08f29289",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "', IN Allegis banks 348 said April intervention total year'"
      ]
     },
     "execution_count": 653,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(uni_grams, 1, (), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7cf38a1-2386-43a8-9e2d-98ed5d0e0731",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_probs(grams):\n",
    "    # Let's transform the counts to probabilities\n",
    "    for context in grams:\n",
    "        total_count = float(sum(grams[context].values()))\n",
    "        for next_word in grams[context]:\n",
    "            grams[context][next_word] /= total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afae1772-f6e7-418f-a16c-efc57624a945",
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_grams = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "for sentence in reuters.sents():\n",
    "    for w1, w2 in bigrams(sentence, pad_right=True, pad_left=True):\n",
    "        bi_grams[(w1)][w2] += 1 \n",
    "\n",
    "calc_probs(bi_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8eee6728-2834-4d2f-ac45-c59950639cda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The company in its pretax securities by protectionist bills or sale'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(bi_grams, 2, ('The',), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc2c916-6f19-4a3b-ae84-ec819d21dc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "tri_grams = defaultdict(lambda: defaultdict(lambda: 0))\n",
    " \n",
    "for sentence in reuters.sents():\n",
    "    for w1, w2, w3 in trigrams(sentence, pad_right=True, pad_left=True):\n",
    "        tri_grams[(w1, w2)][w3] += 1\n",
    " \n",
    "for w1_w2 in tri_grams:\n",
    "    total_count = float(sum(tri_grams[w1_w2].values()))\n",
    "    for w3 in tri_grams[w1_w2]:\n",
    "        tri_grams[w1_w2][w3] /= total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "id": "6bb9589d-1ebe-4f7b-ab5f-48625041099e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'None The present five - year Canadian bonds only about 760 oil'"
      ]
     },
     "execution_count": 660,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(tri_grams, 3, (None, 'The'), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "48550100-fe86-43b8-9835-d647ae10e12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0b9e7a25-b52a-4773-84c2-d6afe51e931a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp_params = {\n",
    "    \"batch_size\": 32,\n",
    "    \"embedding_dim\": 125,\n",
    "    \"hidden_dim\": 2,\n",
    "    \"sequence_len\": 10\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "cf68eabb-3e74-416d-af96-81afa54752a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMDataset(Dataset):\n",
    "    def __init__(self, nltk_corpus, sentence_window = 50, train_vocab=None):\n",
    "        self.corpus = nltk.tokenize.wordpunct_tokenize(nltk_corpus)\n",
    "        self.vocab = train_vocab if train_vocab else self._build_vocab()\n",
    "        self.seq_len = sentence_window\n",
    "        \n",
    "        self.slider = -1\n",
    "\n",
    "    def __len__(self):\n",
    "        return math.floor(len(self.corpus)/self.seq_len)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        \n",
    "        self.slider += 1\n",
    "        \n",
    "        src_text_tokens = self.corpus[self.slider * self.seq_len : (self.slider + 1) * self.seq_len]\n",
    "        trg_text_tokens = self.corpus[(self.slider * self.seq_len) + 1 : ((self.slider + 1) * self.seq_len) + 1]\n",
    "        \n",
    "        \n",
    "        return {\n",
    "            'src': self.vocab.lookup_indices(src_text_tokens),\n",
    "            'trg': self.vocab.lookup_indices(trg_text_tokens)\n",
    "        }\n",
    "    \n",
    "    def _build_vocab(self):\n",
    "        vocab = build_vocab_from_iterator([self.corpus], specials=[\"<unk>\",\"<pad>\"])\n",
    "        vocab.set_default_index(vocab['<unk>'])\n",
    "\n",
    "        return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e53b9d5e-bab1-413d-8e18-cdaf1e96bbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch, pad_value, device):\n",
    "    trgs = []\n",
    "    srcs = []\n",
    "    for row in batch:        \n",
    "        srcs.append(torch.tensor(row[\"src\"], dtype=torch.long).to(device))\n",
    "        trgs.append(torch.tensor(row[\"trg\"]).to(device))\n",
    "\n",
    "    padded_srcs = pad_sequence(srcs, padding_value=pad_value)\n",
    "    padded_trgs = pad_sequence(trgs, padding_value=pad_value)\n",
    "    return {\"src\": padded_srcs, \"trg\": padded_trgs}\n",
    "\n",
    "train_lmds = LMDataset(reuters.raw(), hyp_params[\"sequence_len\"])\n",
    "\n",
    "pad_value = train_lmds.vocab['<pad>']\n",
    "\n",
    "train_dt = DataLoader(train_lmds, \n",
    "                      batch_size=hyp_params[\"batch_size\"], \n",
    "                      shuffle=True,\n",
    "                      collate_fn=lambda batch_size: collate_fn(batch_size, pad_value, device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "364171c6-2654-473a-abde-56cf17db5bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LM(nn.Module):\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim):\n",
    "        super(LM, self).__init__()\n",
    "\n",
    "\n",
    "        # Embedding is just an lookup table of size \"vocab_size\"\n",
    "        # and each element has \"embedding_size\" dimension\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.LSTM = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Shape --> [Sequence_length , batch_size , embedding dims]\n",
    "        embedding = self.embedding(x)\n",
    "        # Shape --> (output) [Sequence_length , batch_size , hidden_size]\n",
    "        # Shape --> (hs, cs) [num_layers, batch_size size, hidden_size]\n",
    "        outputs, (hidden_state, cell_state) = self.LSTM(embedding)\n",
    "        \n",
    "        '''\n",
    "            Unlike Classification task, \n",
    "            here we are making use of outputs from our LSTM.\n",
    "        '''\n",
    "        \n",
    "        linear_outputs = self.fc(outputs)\n",
    "        \n",
    "        return linear_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4540af57-d158-474d-b7cc-8224756a4209",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LM(len(train_lmds.vocab), len(train_lmds.vocab), hyp_params[\"embedding_dim\"], hyp_params[\"hidden_dim\"]).to(device)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    print('Epoch: ', epoch)\n",
    "    for idx, batch in enumerate(tqdm(train_dt)):\n",
    "        src = batch[\"src\"]  # shape --> e.g. (10, 32) sentence len, batch size\n",
    "        trg = batch[\"trg\"]  # shape --> e.g. (10, 32) sentence len, batch size\n",
    "\n",
    "        # Clear the accumulating gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # shape --> (10, 32, 41602) sentence len, batch size, trg vocab\n",
    "        output = model(src)\n",
    "\n",
    "        # Calculate the loss value for every epoch\n",
    "        \n",
    "        loss = criterion(output.view(-1, len(train_lmds.vocab)), trg.view(-1))\n",
    "\n",
    "        # Calculate the gradients for weights & biases using back-propagation\n",
    "        loss.backward()\n",
    "\n",
    "        epoch_loss += loss.detach().cpu()\n",
    "\n",
    "        # Clip the gradient value is it exceeds > 1\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "\n",
    "        # Update the weights values\n",
    "        optimizer.step()\n",
    "    print('\\tTrain loss: ', epoch_loss/len(train_dt))\n",
    "    train_lmds.slider = -1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
