{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "567156ca-c204-40fd-b444-5324e9ac0570",
   "metadata": {},
   "source": [
    "# Language Models\n",
    "\n",
    "One of the important and almost backbone of different NLP related tasks!\n",
    "\n",
    "`# TODO: Add more details`\n",
    "\n",
    "We will explore statistical and machine learning methods!\n",
    "Lets starts with statistical methods first!\n",
    "\n",
    "### Statistical method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "878eaccc-0046-4566-b4a7-548ef75bf74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thanks https://nlpforhackers.io/language-models/\n",
    "import random\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "from nltk import bigrams, trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba8f499-decb-47b4-84c2-c8096784a3d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/maqboolkhan/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('reuters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13f198b0-0825-4d14-a389-caea81d5ada1",
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_grams = Counter(reuters.words())\n",
    "total_count = len(reuters.words())\n",
    " \n",
    "\n",
    "# Compute the probabilities (uni-grams)\n",
    "for word in uni_grams:\n",
    "    uni_grams[word] /= float(total_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "125f31ba-f033-409f-b112-81b5660042d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('.', 0.055021758950689205),\n",
       " (',', 0.042047741270415905),\n",
       " ('the', 0.033849129031826936),\n",
       " ('of', 0.02090707135390124),\n",
       " ('to', 0.01977743054365126),\n",
       " ('in', 0.015386126221089999),\n",
       " ('said', 0.014657438167564549),\n",
       " ('and', 0.014552260705293332),\n",
       " ('a', 0.013650988639090802),\n",
       " ('mln', 0.010481137497159917)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uni_grams_counter = Counter(uni_grams)\n",
    "uni_grams_counter.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bdaaae21-afdc-4ef3-bcfc-0ab95347eb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(grams, n, context, length):\n",
    "    text = list(context)\n",
    "    context = context[0] if n == 2 else context # bigrams had , in tne context tuple hence to remedy that comma!\n",
    "    \n",
    "    for i in range(length):\n",
    "        sum = 0\n",
    "        \n",
    "        r = random.random()\n",
    "        \n",
    "        if context:\n",
    "            candidates = grams[context]\n",
    "        else:\n",
    "            candidates = grams\n",
    "        \n",
    "    \n",
    "        for k in candidates.keys():\n",
    "            sum += candidates[k]\n",
    "            if sum > r:\n",
    "                text.append(k)\n",
    "                \n",
    "                if context:\n",
    "                    context = (k) if n == 2 else (context[2-n], k)\n",
    "                \n",
    "                break\n",
    "    text = ['None' if token == None else token  for token in text] # Replacing None with 'None'\n",
    "    return ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "id": "85755ffb-ba9c-4208-ba04-4c2e08f29289",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "', IN Allegis banks 348 said April intervention total year'"
      ]
     },
     "execution_count": 653,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(uni_grams, 1, (), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "d7cf38a1-2386-43a8-9e2d-98ed5d0e0731",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_probs(grams):\n",
    "    # Let's transform the counts to probabilities\n",
    "    for context in grams:\n",
    "        total_count = float(sum(grams[context].values()))\n",
    "        for next_word in grams[context]:\n",
    "            grams[context][next_word] /= total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "afae1772-f6e7-418f-a16c-efc57624a945",
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_grams = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "for sentence in reuters.sents():\n",
    "    for w1, w2 in bigrams(sentence, pad_left=True, left_pad_symbol=\"<s>\", pad_right=True, right_pad_symbol=\"</s>\"):\n",
    "        bi_grams[(w1)][w2] += 1 \n",
    "\n",
    "calc_probs(bi_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "8eee6728-2834-4d2f-ac45-c59950639cda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Federal Savings System Inc and Japanese intervention ,\" he added'"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(bi_grams, 2, ('The',), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "ffc2c916-6f19-4a3b-ae84-ec819d21dc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "tri_grams = defaultdict(lambda: defaultdict(lambda: 0))\n",
    " \n",
    "for sentence in reuters.sents():\n",
    "    for w1, w2, w3 in trigrams(sentence, pad_left=True, left_pad_symbol=\"<s>\", pad_right=True, right_pad_symbol=\"</s>\"):\n",
    "        tri_grams[(w1, w2)][w3] += 1\n",
    "    \n",
    "calc_probs(tri_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "6bb9589d-1ebe-4f7b-ab5f-48625041099e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> The bonus award was made during the morning after analyst Daniel'"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(tri_grams, 3, ('<s>', 'The'), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2811a085-748b-4fd3-8bff-8c90f408f52c",
   "metadata": {},
   "source": [
    "### Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "a5e3ad03-df27-4f5b-b055-19e469c4c096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uni gram perplexity:  1077.8271341207844\n"
     ]
    }
   ],
   "source": [
    "# https://stats.stackexchange.com/a/143638/291743\n",
    "N = 0\n",
    "summation = 0\n",
    "for word in nltk.tokenize.wordpunct_tokenize(reuters.raw()):\n",
    "    N += 1\n",
    "    summation += math.log(uni_grams[word], 2)\n",
    "\n",
    "print(\"Uni gram perplexity: \", pow(2, -summation * (1/N)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "03fcc6d3-04cd-4173-ad84-ff5d1963c390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bi gram perplexity:  40.65095064037762\n"
     ]
    }
   ],
   "source": [
    "# https://towardsdatascience.com/perplexity-in-language-models-87a196019a94\n",
    "N = 0\n",
    "summation = 0\n",
    "for sentence in reuters.sents():\n",
    "    N += len(sentence) + 2 # +2 for <s> and </s> \n",
    "    for w1, w2 in bigrams(sentence, pad_left=True, left_pad_symbol=\"<s>\", pad_right=True, right_pad_symbol=\"</s>\"): \n",
    "        summation += math.log(bi_grams[(w1)][w2], 2)\n",
    "print(\"Bi gram perplexity: \", pow(2, -summation * (1/N)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "c4e795c3-59b8-4ef1-83ea-2fa805003173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tri gram perplexity:  5.9633012418563265\n"
     ]
    }
   ],
   "source": [
    "N = 0\n",
    "summation = 0\n",
    "for sentence in reuters.sents():\n",
    "    N += len(sentence) + 4 # +4 for <s> and </s> \n",
    "    for w1, w2, w3 in trigrams(sentence, pad_left=True, left_pad_symbol=\"<s>\", pad_right=True, right_pad_symbol=\"</s>\"): \n",
    "        summation += math.log(tri_grams[(w1, w2)][w3], 2)\n",
    "print(\"Tri gram perplexity: \", pow(2, -summation * (1/N)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "48550100-fe86-43b8-9835-d647ae10e12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b9e7a25-b52a-4773-84c2-d6afe51e931a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp_params = {\n",
    "    \"batch_size\": 32,\n",
    "    \"embedding_dim\": 125,\n",
    "    \"hidden_dim\": 2,\n",
    "    \"sequence_len\": 10\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf68eabb-3e74-416d-af96-81afa54752a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMDataset(Dataset):\n",
    "    def __init__(self, nltk_corpus, sentence_window = 50, train_vocab=None):\n",
    "        self.corpus = nltk.tokenize.wordpunct_tokenize(nltk_corpus)\n",
    "        self.vocab = train_vocab if train_vocab else self._build_vocab()\n",
    "        self.seq_len = sentence_window\n",
    "        \n",
    "        self.slider = -1\n",
    "\n",
    "    def __len__(self):\n",
    "        return math.floor(len(self.corpus)/self.seq_len)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        \n",
    "        self.slider += 1\n",
    "        \n",
    "        src_text_tokens = self.corpus[self.slider * self.seq_len : (self.slider + 1) * self.seq_len]\n",
    "        trg_text_tokens = self.corpus[(self.slider * self.seq_len) + 1 : ((self.slider + 1) * self.seq_len) + 1]\n",
    "        \n",
    "        \n",
    "        return {\n",
    "            'src': self.vocab.lookup_indices(src_text_tokens),\n",
    "            'trg': self.vocab.lookup_indices(trg_text_tokens)\n",
    "        }\n",
    "    \n",
    "    def _build_vocab(self):\n",
    "        vocab = build_vocab_from_iterator([self.corpus], specials=[\"<unk>\",\"<pad>\"])\n",
    "        vocab.set_default_index(vocab['<unk>'])\n",
    "\n",
    "        return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e53b9d5e-bab1-413d-8e18-cdaf1e96bbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch, pad_value, device):\n",
    "    trgs = []\n",
    "    srcs = []\n",
    "    for row in batch:        \n",
    "        srcs.append(torch.tensor(row[\"src\"], dtype=torch.long).to(device))\n",
    "        trgs.append(torch.tensor(row[\"trg\"]).to(device))\n",
    "\n",
    "    padded_srcs = pad_sequence(srcs, padding_value=pad_value)\n",
    "    padded_trgs = pad_sequence(trgs, padding_value=pad_value)\n",
    "    return {\"src\": padded_srcs, \"trg\": padded_trgs}\n",
    "\n",
    "train_lmds = LMDataset(reuters.raw(), hyp_params[\"sequence_len\"])\n",
    "\n",
    "pad_value = train_lmds.vocab['<pad>']\n",
    "\n",
    "train_dt = DataLoader(train_lmds, \n",
    "                      batch_size=hyp_params[\"batch_size\"], \n",
    "                      shuffle=True,\n",
    "                      collate_fn=lambda batch_size: collate_fn(batch_size, pad_value, device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "364171c6-2654-473a-abde-56cf17db5bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(LM, self).__init__()\n",
    "\n",
    "\n",
    "        # Embedding is just an lookup table of size \"vocab_size\"\n",
    "        # and each element has \"embedding_size\" dimension\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.LSTM = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden_state=None, cell_state=None):\n",
    "        # Shape --> [Sequence_length , batch_size , embedding dims]\n",
    "        embedding = self.embedding(x)\n",
    "        # Shape --> (output) [Sequence_length , batch_size , hidden_size]\n",
    "        # Shape --> (hs, cs) [num_layers, batch_size size, hidden_size]\n",
    "        if hidden_state != None:\n",
    "            outputs, (hidden_state, cell_state) = self.LSTM(embedding, (hidden_state, cell_state))\n",
    "        else:\n",
    "            outputs, (hidden_state, cell_state) = self.LSTM(embedding)\n",
    "        \n",
    "        '''\n",
    "            Unlike Classification task, \n",
    "            here we are making use of outputs from our LSTM.\n",
    "        '''\n",
    "        # shape --> (linear_outputs) (10, 32, 41602) sentence len, batch size, vocab size\n",
    "        linear_outputs = self.fc(outputs)\n",
    "        \n",
    "        return linear_outputs, hidden_state, cell_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "ac2f0fc7-581d-4591-bc24-e4a73de40e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LM(len(train_lmds.vocab), hyp_params[\"embedding_dim\"], hyp_params[\"hidden_dim\"]).to(device)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4540af57-d158-474d-b7cc-8224756a4209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5378/5378 [01:58<00:00, 45.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain loss:  7.6194728451973885\n",
      "Epoch:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5378/5378 [01:54<00:00, 46.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain loss:  6.86116912720504\n",
      "Epoch:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5378/5378 [02:00<00:00, 44.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain loss:  6.690379174237093\n",
      "Epoch:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5378/5378 [01:56<00:00, 46.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain loss:  6.571957478517935\n",
      "Epoch:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5378/5378 [01:56<00:00, 46.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain loss:  6.48696317208128\n",
      "Epoch:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5378/5378 [01:56<00:00, 46.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain loss:  6.415047833892039\n",
      "Epoch:  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5378/5378 [01:56<00:00, 46.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain loss:  6.354719042556354\n",
      "Epoch:  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5378/5378 [01:57<00:00, 45.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain loss:  6.306523078699988\n",
      "Epoch:  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5378/5378 [01:56<00:00, 46.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain loss:  6.269592829910341\n",
      "Epoch:  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5378/5378 [01:58<00:00, 45.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain loss:  6.2395854212618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    print('Epoch: ', epoch)\n",
    "    for idx, batch in enumerate(tqdm(train_dt)):\n",
    "        src = batch[\"src\"]  # shape --> e.g. (10, 32) sentence len, batch size\n",
    "        trg = batch[\"trg\"]  # shape --> e.g. (10, 32) sentence len, batch size\n",
    "        \n",
    "        trg = trg.view(-1) # making them linear (1d) --> bsz * seq len\n",
    " \n",
    "        # Clear the accumulating gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # shape --> (10, 32, 41602) sentence len, batch size, vocab\n",
    "        output = model(src)\n",
    "        \n",
    "        # Calculate the loss value for every epoch\n",
    "        loss = criterion(output.view(-1, len(train_lmds.vocab)), trg)\n",
    "\n",
    "        # Calculate the gradients for weights & biases using back-propagation\n",
    "        loss.backward()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # Clip the gradient value is it exceeds > 1\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "\n",
    "        # Update the weights values\n",
    "        optimizer.step()\n",
    "    print(f'\\tTrain loss: {epoch_loss/len(train_dt)}, Train perplexity: {math.exp(epoch_loss/len(train_dt))}')\n",
    "    train_lmds.slider = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c230e4a-0237-4b3a-8f43-29aaa11a69a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),'lm.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "74b9d617-3a61-4caf-9527-b7b528e4f191",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LM(\n",
       "  (embedding): Embedding(41602, 125)\n",
       "  (LSTM): LSTM(125, 2)\n",
       "  (fc): Linear(in_features=2, out_features=41602, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_model = torch.load('lm.pt', map_location=device)\n",
    "model.load_state_dict(pre_model)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "1676f8e1-5084-4b6f-b646-4409f23e5a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bank and a market - Europe visible expected its third "
     ]
    }
   ],
   "source": [
    "inp = \"The\"\n",
    "\n",
    "hs = None\n",
    "cs = None\n",
    "for i in range(10):\n",
    "    inp_ind = torch.tensor([train_lmds.vocab[inp]]).unsqueeze(1)\n",
    "    output, hs, cs = model(inp_ind, hs, cs)\n",
    "    word_weights = output.squeeze().exp().cpu()\n",
    "    word_idx = torch.multinomial(word_weights, 1)[0]\n",
    "    inp = train_lmds.vocab.lookup_token(word_idx)\n",
    "    print(inp, end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5664265b-6311-4662-a62e-3147f4105200",
   "metadata": {},
   "source": [
    "### Calculating perplexity of a single sentence\n",
    "But we only have input? How to manage it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "8222a107-ab11-4c9e-826b-71aa70ad6471",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "822.3764490590262"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Thanks: https://github.com/flairNLP/flair/issues/498#issuecomment-465192107\n",
    "sentence = reuters.sents()[0]\n",
    "\n",
    "# this was the main step\n",
    "inp = sentence[:-1]\n",
    "trg = sentence[1:]\n",
    "\n",
    "trg_tensor = torch.tensor(train_lmds.vocab.lookup_indices(trg))\n",
    "\n",
    "inp_tensor = torch.tensor(train_lmds.vocab.lookup_indices(inp)).unsqueeze(1)\n",
    "output, _, _ = model(inp_tensor)\n",
    "\n",
    "loss = criterion(output.view(-1, len(train_lmds.vocab)), trg_tensor).item()\n",
    "\n",
    "math.exp(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bab3b4f-bebf-4d09-b703-84c3110db753",
   "metadata": {},
   "outputs": [],
   "source": [
    "hs = None\n",
    "cs = None\n",
    "sentence = reuters.sents()[0]\n",
    "probs = 0\n",
    "for idx in range(len(sentence)):\n",
    "    inp_ind = torch.tensor(train_lmds.vocab.lookup_indices(sentence[:idx+1])).unsqueeze(1)\n",
    "    output, _, _ = model(inp_ind)\n",
    "    output = output.squeeze().detach()\n",
    "    if idx > 0:\n",
    "        # Getting the last one (so far probabilities)\n",
    "        output = output[idx]\n",
    "    \n",
    "    probs += output[train_lmds.vocab[sentence[idx]]].item()\n",
    "\n",
    "print(probs/len(sentence))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
