{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "567156ca-c204-40fd-b444-5324e9ac0570",
   "metadata": {},
   "source": [
    "# Language Models\n",
    "\n",
    "One of the important and almost the backbone of different NLP related tasks! \n",
    "Let's assume we want to calculate the probability of a sentence  $P(S) = P(W_1 , â€¦ , W_m)$ or if we want to predict the next word given the probabilities of previous words $P(W_n|W_1 , â€¦ , W_m)$ then Language Model is the task which does it. Now, while calculating probability of next word given previous words usually we have to look back so far (in ideal situation, all the previous words). Thankfully, we use `Markov assumption` and only consider few words from the past. \n",
    "So, in this notebook we will first develop statistical and then Deep Learning approaches.\n",
    "\n",
    "Let's starts statistical methods. I mentioned about consideration of previous words, this is where the concept of `n-grams` kicks in. N in `n-grams` specifies number of words we want to look back for the prediction of next words! So how does this `n-grams` works?\n",
    "Suppose we want to generate a sentence from one single word `The`! then first step would be to find the most probable word after the word `The` and to find this most probable word we have to calculate probalities of all the words from the corpus given the word `The`. Trust me, it's not difficult as it sounds! \n",
    "Let's consider an example. Suppose we want to calculate the probability of word `company` occuring after the given word `The` $P(Company|The)$. So, we just need to calculate how many times `The company` occured in our corpus and then just normalized this count with number of times word `The` occured in our courpus. Hence, complete forumlar would be\n",
    "\n",
    "$$P(Company|The) = \\frac{count(The, Company)}{count(The)}$$\n",
    "\n",
    "We will use same formula for all the words in our corpus and our next predicted word would be simply the word with highest probability. Hence, the more general form of above given formula can be written as \n",
    "\n",
    "$$P(W_i|W_{i-1}) = \\frac{count(W_{i-1}, W_i)}{count(W_{i-1})}$$\n",
    "\n",
    "When we consider single word from the past to predict next word is known as `bi-grams` approach for language modelling as we are considering two `(bi)` words. One thing to remember is that the more words we consider from the past the more better results we will get (you will see yourself in this notebook). So if you want to consider three words from the past we call them `tri-grams`. For `tri-grams` our forumula would simply include one more word.\n",
    "\n",
    "$$P(c|a,b) = \\frac{count(a, b, c)}{count(a,b)}$$\n",
    "\n",
    "But there is one problem, if you want to use more previous words from the past which is `Sparsity` !. For example, from last formula what if we simply cant count $count(a, b, c)$ or $count(a, b)$ because these words never occured together! Then we have two solutions:\n",
    "\n",
    "1. Smoothing \n",
    "2. Backoff (if tri-gram is not avialable then use bi-gram if not then uni-gram)\n",
    "\n",
    "I would simply ignore these problems in this notebook ðŸ˜¬. However, at the end of this notebook I have provided helpful resources to study more about them. Another thing is, for every Artificial Intelligence problem or task we must evaluate our models on seperate validation and test sets. Here I am completely ignoring this as this notebook is more about understanding and implementation of LMs. Although, if you are going to use LMs in real scenarios you must use validation set.\n",
    "\n",
    "### Statistical method\n",
    "\n",
    "Enough talk let's start coding. We will use `nltk` as it simplfies creation of `bi-grams` and `tri-grams`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "878eaccc-0046-4566-b4a7-548ef75bf74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thanks https://nlpforhackers.io/language-models/\n",
    "import random\n",
    "import math\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "from nltk import bigrams, trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba8f499-decb-47b4-84c2-c8096784a3d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/maqboolkhan/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('reuters')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae735ba2-a863-4b52-af6f-41509e96b80f",
   "metadata": {},
   "source": [
    "However, before coding `bi-grams` there is another appproach known as `uni-gram`. In `uni-gram` we don't consider any previous word. Hence, we only count occurance of each word and then simply normalized (divide) this count with total no. of words in the corpus. This approach also known as `bag-of-words`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13f198b0-0825-4d14-a389-caea81d5ada1",
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_grams = Counter(reuters.words())  # Counting occurance of each word in our corpus\n",
    "total_count = len(reuters.words())\n",
    "\n",
    "# Compute the probabilities (uni-grams)\n",
    "for word in uni_grams:\n",
    "    uni_grams[word] /= float(total_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b4b429-674c-4661-adf2-74828fd4a406",
   "metadata": {},
   "source": [
    "Now, as we have calculated `uni-grams`. Let's just check most occured tokens in our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "125f31ba-f033-409f-b112-81b5660042d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('.', 0.055021758950689205),\n",
       " (',', 0.042047741270415905),\n",
       " ('the', 0.033849129031826936),\n",
       " ('of', 0.02090707135390124),\n",
       " ('to', 0.01977743054365126),\n",
       " ('in', 0.015386126221089999),\n",
       " ('said', 0.014657438167564549),\n",
       " ('and', 0.014552260705293332),\n",
       " ('a', 0.013650988639090802),\n",
       " ('mln', 0.010481137497159917)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uni_grams_counter = Counter(uni_grams)\n",
    "uni_grams_counter.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26189f00-5d73-4932-b3c4-2f1c257275cb",
   "metadata": {},
   "source": [
    "Now, here comes most excited part!! lets generate language. `Uni-grams` do not consider any previous word so now it's completely upto us how we pick next word. Here, what I did is to first get random number (float) and then iterate over all the words and sum their probabilities and as soon as this sum is greater than that random number we pick the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bdaaae21-afdc-4ef3-bcfc-0ab95347eb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(grams, n, context, length):\n",
    "    text = list(context)\n",
    "    context = context[0] if n == 2 else context  # bigrams had , in tne context tuple hence to remedy that comma!\n",
    "\n",
    "    for i in range(length):\n",
    "        sum = 0\n",
    "\n",
    "        r = random.random()  # For diversity in text generation\n",
    "\n",
    "        if context:\n",
    "            candidates = grams[context]\n",
    "        else:\n",
    "            candidates = grams\n",
    "\n",
    "        for k in candidates.keys():\n",
    "            sum += candidates[k]\n",
    "            if sum > r:\n",
    "                text.append(k)\n",
    "\n",
    "                if context:\n",
    "                    context = (k) if n == 2 else (context[2-n], k)\n",
    "\n",
    "                break\n",
    "    text = ['None' if token == None else token  for token in text] # Replacing None with 'None'\n",
    "    return ' '.join(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d84b09-420c-4489-961a-747b1126ce33",
   "metadata": {},
   "source": [
    "Now, lets generate some text using our `uni-grams` model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "id": "85755ffb-ba9c-4208-ba04-4c2e08f29289",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "', IN Allegis banks 348 said April intervention total year'"
      ]
     },
     "execution_count": 653,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(uni_grams, 1, (), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e97c88-d281-4b49-8834-a8353ac8437e",
   "metadata": {},
   "source": [
    "hmm, not very impressive. Let's move on to `bi-grams`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "d7cf38a1-2386-43a8-9e2d-98ed5d0e0731",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_probs(grams):\n",
    "    # Let's transform the counts to probabilities\n",
    "    for context in grams:\n",
    "        context_count = float(sum(grams[context].values()))\n",
    "        for next_word in grams[context]:\n",
    "            grams[context][next_word] /= context_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "afae1772-f6e7-418f-a16c-efc57624a945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting all bi-grams\n",
    "bi_grams = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "for sentence in reuters.sents():\n",
    "    for w1, w2 in bigrams(sentence, pad_left=True, left_pad_symbol=\"<s>\", pad_right=True, right_pad_symbol=\"</s>\"):\n",
    "        bi_grams[(w1)][w2] += 1\n",
    "\n",
    "# Calculating probabilities\n",
    "calc_probs(bi_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "8eee6728-2834-4d2f-ac45-c59950639cda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Federal Savings System Inc and Japanese intervention ,\" he added'"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(bi_grams, 2, ('The',), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2877c89-4dd3-4202-aa5a-d84cdebe2fbe",
   "metadata": {},
   "source": [
    "Notice here we are passing previous word (context) and it's single word as we are using `bi-grams`.  Also notice that the generated sentence is coherent.\n",
    "Now, let's try with `tri-grams`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "ffc2c916-6f19-4a3b-ae84-ec819d21dc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "tri_grams = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "\n",
    "for sentence in reuters.sents():\n",
    "    for w1, w2, w3 in trigrams(sentence, pad_left=True, left_pad_symbol=\"<s>\", pad_right=True, right_pad_symbol=\"</s>\"):\n",
    "        tri_grams[(w1, w2)][w3] += 1\n",
    "\n",
    "calc_probs(tri_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "6bb9589d-1ebe-4f7b-ab5f-48625041099e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> The bonus award was made during the morning after analyst Daniel'"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(tri_grams, 3, ('<s>', 'The'), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2811a085-748b-4fd3-8bff-8c90f408f52c",
   "metadata": {},
   "source": [
    "### Perplexity\n",
    "\n",
    "Now, we have understood language models and how to generate text with them. However, one important aspect of any ML model is how to evaluate them. For this task, we can ask does our language model prefer good sentences to bad ones? does our model assign a higher probability to real or frequently observed word than ungrammatical or rarely observed one? Other than that we also have automatic measure which is `Perplexity`. It's inverse probability of our corpus (or sentence we can also calculate it for sentence) normalized by no. of words in our corpus. \n",
    "\n",
    "$$\\sqrt[N]{ \\frac{1}{P(w_1 ... w_n)}}$$\n",
    "\n",
    "Notice here we have to calculate $P(w_1 ... w_n)$ which involve _Chain Rule of Probability_ e.g  $P(w_1 ... w_n) = P(w_1) x P(w_2|w_1) x P(w_3|w_1, w_2) ... $ and notice if one of these probabilities is zero then the whole $P(w_1 ... w_n)$ will become zero. To avoid this underflow, I will use alternate representation (formula) of perplexity involving _logs_.\n",
    "\n",
    "$$2^{-\\frac{1}{N} \\sum_{i=1}^{N} log_2 P(w_i) }$$\n",
    "\n",
    "As perplexity is inverse measure. Hence, lower Perplexity indicates better language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "a5e3ad03-df27-4f5b-b055-19e469c4c096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uni gram perplexity:  1077.8271341207844\n"
     ]
    }
   ],
   "source": [
    "# https://stats.stackexchange.com/a/143638/291743\n",
    "N = 0\n",
    "summation = 0\n",
    "for word in nltk.tokenize.wordpunct_tokenize(reuters.raw()):\n",
    "    N += 1\n",
    "    summation += math.log(uni_grams[word], 2)\n",
    "\n",
    "print(\"Uni gram perplexity: \", pow(2, -summation * (1/N)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "03fcc6d3-04cd-4173-ad84-ff5d1963c390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bi gram perplexity:  40.65095064037762\n"
     ]
    }
   ],
   "source": [
    "# https://towardsdatascience.com/perplexity-in-language-models-87a196019a94\n",
    "N = 0\n",
    "summation = 0\n",
    "for sentence in reuters.sents():\n",
    "    N += len(sentence) + 2 # +2 for <s> and </s> \n",
    "    for w1, w2 in bigrams(sentence, pad_left=True, left_pad_symbol=\"<s>\", pad_right=True, right_pad_symbol=\"</s>\"): \n",
    "        summation += math.log(bi_grams[(w1)][w2], 2)\n",
    "print(\"Bi gram perplexity: \", pow(2, -summation * (1/N)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "c4e795c3-59b8-4ef1-83ea-2fa805003173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tri gram perplexity:  5.9633012418563265\n"
     ]
    }
   ],
   "source": [
    "N = 0\n",
    "summation = 0\n",
    "for sentence in reuters.sents():\n",
    "    N += len(sentence) + 4 # +4 for <s> and </s> \n",
    "    for w1, w2, w3 in trigrams(sentence, pad_left=True, left_pad_symbol=\"<s>\", pad_right=True, right_pad_symbol=\"</s>\"): \n",
    "        summation += math.log(tri_grams[(w1, w2)][w3], 2)\n",
    "print(\"Tri gram perplexity: \", pow(2, -summation * (1/N)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ab383a-e7a6-4665-966e-b7c5479aa315",
   "metadata": {},
   "source": [
    "So as I said earlier, that the more words we consider from the past the more better results we will get and one can see it here. \n",
    "Now, it's time to do the same task of Language Modeling with Deep Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48550100-fe86-43b8-9835-d647ae10e12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b9e7a25-b52a-4773-84c2-d6afe51e931a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp_params = {\n",
    "    \"batch_size\": 32,\n",
    "    \"embedding_dim\": 125,\n",
    "    \"hidden_dim\": 2,\n",
    "    \"sequence_len\": 10\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3551ffc2-4b46-480d-bfd1-979a5e08a74d",
   "metadata": {},
   "source": [
    "We will use simple vanilla LSTM model. One important thing to understand here is how our input to LSTM model and output will look like. We will use _window mechanism_ which is simillar as `n-gram` approach. Let's consider an example, suppose we have a sentence \"The quick brown fox jumps over the lazy dog\" and we set the size of input and output _sentence window_ is `3` hence our first _input_ would be \"The quick brown\" and output would be \"quick brown fox\". Following the same approach, the second input would be \"quick brown fox\" and output would be \"brown fox jumps\". The next `LMDataset` class implementd this logic and it takes `sentence_window` as its input. If you look _`__getitem__`_ method of `LMDataset` class uses this `sentence_window` and _`slider`_ to create our input and output as I explained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf68eabb-3e74-416d-af96-81afa54752a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMDataset(Dataset):\n",
    "    def __init__(self, nltk_corpus, sentence_window = 50, train_vocab=None):\n",
    "        self.corpus = nltk.tokenize.wordpunct_tokenize(nltk_corpus)\n",
    "        self.vocab = train_vocab if train_vocab else self._build_vocab()\n",
    "        self.sentence_window = sentence_window\n",
    "\n",
    "        self.slider = -1\n",
    "\n",
    "    def __len__(self):\n",
    "        return math.floor(len(self.corpus)/self.sentence_window)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        self.slider += 1\n",
    "\n",
    "        src_text_tokens = self.corpus[self.slider * self.sentence_window : (self.slider + 1) * self.sentence_window]\n",
    "        trg_text_tokens = self.corpus[(self.slider * self.sentence_window) + 1 : ((self.slider + 1) * self.sentence_window) + 1]\n",
    "        \n",
    "        return {\n",
    "            'src': self.vocab.lookup_indices(src_text_tokens),\n",
    "            'trg': self.vocab.lookup_indices(trg_text_tokens)\n",
    "        }\n",
    "\n",
    "    def _build_vocab(self):\n",
    "        vocab = build_vocab_from_iterator([self.corpus], specials=[\"<unk>\",\"<pad>\"])\n",
    "        vocab.set_default_index(vocab['<unk>'])\n",
    "\n",
    "        return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e53b9d5e-bab1-413d-8e18-cdaf1e96bbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch, pad_value, device):\n",
    "    trgs = []\n",
    "    srcs = []\n",
    "    for row in batch:\n",
    "        srcs.append(torch.tensor(row[\"src\"], dtype=torch.long).to(device))\n",
    "        trgs.append(torch.tensor(row[\"trg\"]).to(device))\n",
    "\n",
    "    padded_srcs = pad_sequence(srcs, padding_value=pad_value)\n",
    "    padded_trgs = pad_sequence(trgs, padding_value=pad_value)\n",
    "    return {\"src\": padded_srcs, \"trg\": padded_trgs}\n",
    "\n",
    "\n",
    "train_lmds = LMDataset(reuters.raw(), hyp_params[\"sequence_len\"])\n",
    "\n",
    "pad_value = train_lmds.vocab['<pad>']\n",
    "\n",
    "train_dt = DataLoader(train_lmds, \n",
    "                      batch_size=hyp_params[\"batch_size\"],\n",
    "                      shuffle=True,\n",
    "                      collate_fn=lambda batch_size: collate_fn(batch_size, pad_value, device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "364171c6-2654-473a-abde-56cf17db5bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        # Embedding is just an lookup table of size \"vocab_size\"\n",
    "        # and each element has \"embedding_size\" dimension\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.LSTM = nn.LSTM(embedding_dim, hidden_dim)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden_state=None, cell_state=None):\n",
    "        # Shape --> [Sequence_length , batch_size , embedding dims]\n",
    "        embedding = self.embedding(x)\n",
    "        # Shape --> (output) [Sequence_length , batch_size , hidden_size]\n",
    "        # Shape --> (hs, cs) [num_layers, batch_size size, hidden_size]\n",
    "        if hidden_state is not None:\n",
    "            outputs, (hidden_state, cell_state) = self.LSTM(embedding, (hidden_state, cell_state))\n",
    "        else:\n",
    "            outputs, (hidden_state, cell_state) = self.LSTM(embedding)\n",
    "\n",
    "        '''\n",
    "            Unlike Classification task, \n",
    "            here we are making use of outputs from our LSTM.\n",
    "        '''\n",
    "        # shape --> (linear_outputs) (10, 32, 41602) sentence len, batch size, vocab size\n",
    "        linear_outputs = self.fc(outputs)\n",
    "\n",
    "        return linear_outputs, hidden_state, cell_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac2f0fc7-581d-4591-bc24-e4a73de40e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LM(len(train_lmds.vocab), hyp_params[\"embedding_dim\"], hyp_params[\"hidden_dim\"]).to(device)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac1695a-1f52-45c6-95ce-92fd90c8f44e",
   "metadata": {},
   "source": [
    "Let's train our deep learning model just for 10 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4540af57-d158-474d-b7cc-8224756a4209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5378/5378 [01:57<00:00, 45.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain loss: 7.599785830461212, Train perplexity: 1997.7679882347986\n",
      "Epoch:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5378/5378 [01:56<00:00, 46.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain loss: 6.8328121472752725, Train perplexity: 927.7962470172789\n",
      "Epoch:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5378/5378 [01:56<00:00, 46.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain loss: 6.664019997565886, Train perplexity: 783.6950654540234\n",
      "Epoch:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5378/5378 [01:57<00:00, 45.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain loss: 6.547137107490917, Train perplexity: 697.2451757323084\n",
      "Epoch:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5378/5378 [01:56<00:00, 46.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain loss: 6.4614897322238, Train perplexity: 640.0137959055437\n",
      "Epoch:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5378/5378 [01:57<00:00, 45.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain loss: 6.396214753112282, Train perplexity: 599.571212026656\n",
      "Epoch:  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5378/5378 [01:56<00:00, 46.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain loss: 6.341045046171853, Train perplexity: 567.388949310089\n",
      "Epoch:  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5378/5378 [01:57<00:00, 45.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain loss: 6.289007102569801, Train perplexity: 538.6182708026654\n",
      "Epoch:  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5378/5378 [01:58<00:00, 45.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain loss: 6.245247852141727, Train perplexity: 515.556990997051\n",
      "Epoch:  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5378/5378 [01:58<00:00, 45.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain loss: 6.20877730806506, Train perplexity: 497.09308785487667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    print('Epoch: ', epoch)\n",
    "    for idx, batch in enumerate(tqdm(train_dt)):\n",
    "        src = batch[\"src\"]  # shape --> e.g. (10, 32) sentence len, batch size\n",
    "        trg = batch[\"trg\"]  # shape --> e.g. (10, 32) sentence len, batch size\n",
    "\n",
    "        trg = trg.view(-1) # making them linear (1d) --> bsz * seq len\n",
    "\n",
    "        # Clear the accumulating gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # shape --> (10, 32, 41602) sentence len, batch size, vocab\n",
    "        output, _, _ = model(src)\n",
    "\n",
    "        # Calculate the loss value for every epoch\n",
    "        loss = criterion(output.view(-1, len(train_lmds.vocab)), trg)\n",
    "\n",
    "        # Calculate the gradients for weights & biases using back-propagation\n",
    "        loss.backward()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # Clip the gradient value is it exceeds > 1\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "\n",
    "        # Update the weights values\n",
    "        optimizer.step()\n",
    "    print(f'\\tTrain loss: {epoch_loss/len(train_dt)}, Train perplexity: {math.exp(epoch_loss/len(train_dt))}')\n",
    "    train_lmds.slider = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f7497e-2609-47f0-992a-0c4600b24dd1",
   "metadata": {},
   "source": [
    "So, 10 epochs were not enough even for our _`train`_ dataset as you can see we have very high perplexity infact 10x more than `bi-grams` models. However, I am sure training for more epochs will lead to better results.\n",
    "But how we just calculated `perplexity` using `Cross-entropy` loss ðŸ¤”? So, it turned out there is a relationship between `perplexity` using `Cross-entropy` loss. I am not going to explain here but you can check this article\n",
    "https://towardsdatascience.com/the-relationship-between-perplexity-and-entropy-in-nlp-f81888775ccc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c230e4a-0237-4b3a-8f43-29aaa11a69a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),'lm.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "74b9d617-3a61-4caf-9527-b7b528e4f191",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LM(\n",
       "  (embedding): Embedding(41602, 125)\n",
       "  (LSTM): LSTM(125, 2)\n",
       "  (fc): Linear(in_features=2, out_features=41602, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_model = torch.load('lm.pt', map_location=device)\n",
    "model.load_state_dict(pre_model)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55187ae-4d3f-49ba-bd41-3c89b1624621",
   "metadata": {},
   "source": [
    "### Text generation\n",
    "\n",
    "As Perplexity is very high so we can expect low quality generated text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1676f8e1-5084-4b6f-b646-4409f23e5a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Increased Public support earnings , much of 17 dlrs . "
     ]
    }
   ],
   "source": [
    "inp = \"The\"\n",
    "\n",
    "hs = None\n",
    "cs = None\n",
    "for i in range(10):\n",
    "    inp_ind = torch.tensor([train_lmds.vocab[inp]]).unsqueeze(1)\n",
    "    with torch.no_grad():\n",
    "        output, hs, cs = model(inp_ind, hs, cs)\n",
    "    word_weights = output.squeeze().exp().cpu()\n",
    "    word_idx = torch.multinomial(word_weights, 1)[0]\n",
    "    inp = train_lmds.vocab.lookup_token(word_idx)\n",
    "    print(inp, end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5664265b-6311-4662-a62e-3147f4105200",
   "metadata": {},
   "source": [
    "### Calculating perplexity of a single sentence\n",
    "\n",
    "We already have calculated `perplexity` of whole corpus but what if we want to calculate it for just one sentence ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8222a107-ab11-4c9e-826b-71aa70ad6471",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "795.0305084679243"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Thanks: https://github.com/flairNLP/flair/issues/498#issuecomment-465192107\n",
    "sentence = reuters.sents()[0]\n",
    "\n",
    "# this was the main step\n",
    "inp = sentence[:-1]\n",
    "trg = sentence[1:]\n",
    "\n",
    "trg_tensor = torch.tensor(train_lmds.vocab.lookup_indices(trg)).to(device)\n",
    "\n",
    "inp_tensor = torch.tensor(train_lmds.vocab.lookup_indices(inp)).unsqueeze(1)\n",
    "with torch.no_grad():\n",
    "    output, _, _ = model(inp_tensor)\n",
    "    loss = criterion(output.view(-1, len(train_lmds.vocab)), trg_tensor).item()\n",
    "\n",
    "math.exp(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa29e012-bbae-4492-a763-83744feb266a",
   "metadata": {},
   "source": [
    "### Calculating probability of a single sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5bab3b4f-bebf-4d09-b703-84c3110db753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.002715979320863876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\maq\\Project-NLP\\env\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "hs = None\n",
    "cs = None\n",
    "sentence = reuters.sents()[0]\n",
    "probs = 0\n",
    "for idx in range(len(sentence)):\n",
    "    inp_ind = torch.tensor(train_lmds.vocab.lookup_indices(sentence[:idx+1])).unsqueeze(1)\n",
    "    with torch.no_grad():\n",
    "        output, _, _ = model(inp_ind)\n",
    "    output = F.softmax(output.squeeze().detach())\n",
    "    if idx > 0:\n",
    "        # Getting the last one (so far probabilities)\n",
    "        output = output[idx]\n",
    "    \n",
    "    probs += output[train_lmds.vocab[sentence[idx]]].item()\n",
    "\n",
    "print(probs/len(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fa6ec6-c461-4c80-8604-ef7da66fe68e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
